#  爬虫总结

## 1. 理论知识点

### 1. 反爬手段

```
1.User-Agent(用户代理-UA)
2.代理IP
	西次代理——免费的
	快代理——要钱的
	什么是高匿名、匿名和透明代理？它们有什么区别？
	1.使用透明代理，对方服务器可以知道你使用了代理，并且也知道你的真实IP。
	2.使用匿名代理，对方服务器可以知道你使用了代理，但不知道你的真实IP。
	3.使用高匿名代理，对方服务器不知道你使用了代理，更不知道你的真实IP。
3.验证码访问
	打码平台
		云打码平台
		超级鹰(🦅)
4.动态加载网页  网站返回的是js数据 并不是网页的真实数据 
	selenium驱动真实的浏览器发送请求
5.数据加密  
	分析js代码
```

### 2. HTTP协议

```
1.HTTP：超文本传输协议,端口号80
2.HTTPS：加密传输,端口号443,HTTPS = HTTP + SSL
3.SSL：安全 套接层
	如果报错SSL,解决方案是：
	import urllib.request
		import ssl
		ssl._create_default_https_context = ssl._create_unverified_context	
4.常见服务器端口号
	FTP			21
	SSH			22
	MySQL		3306
	Oracle		1521
	MongoDB 	27017
	Redis		6379
5.url组成
	hhtp://	www.baidu.com:80/	findUser/ ?name=zs&age=18/	#
	协议	  主机IP地址	端口号	   资源路径		参数			锚点	
```

## 2. urllib库

### 1. urllib基本使用

```python
import urllib.request

url = 'http://www.baidu.com'
# 模拟浏览器向服务器发送请求
response = urllib.request.urlopen(url=url)
print(response)
# <http.client.HTTPResponse object at 0x000002A3DBF0F240>
print(type(response))
# <class 'http.client.HTTPResponse'>
```

### 2. 服务器响应

```python
import urllib.request

# read方法	readline方法	readlines方法
# getcode方法	geturl方法	getheaders方法

url = 'http://www.baidu.com'
# 模拟浏览器向服务器发送请求
response = urllib.request.urlopen(url=url)

# 1.read方法
# 以二进制的格式来获取网站源码
print(response.read())
# 读取前5个字节
print(response.read(5))
# 以字符串的格式来读取网站源码
print(response.read().decode('utf-8'))

# 2.readline方法
# 以二进制格式一行一行的读取
a = response.readline()
print(a)
# 读取这一行的前5个字节
a = response.readline(5)
print(a)

# 3.readlines方法
# 一行一行的读取，全部放到列表中
a = response.readlines()
print(a)

# 4.getcode方法
#获取响应的状态码
print(response.getcode())

# 5.geturl方法
# 获取url
print(response.geturl())

# 6.getheaders方法
# 获取请求头信息
print(response.getheaders())
```

### 3. 下载

```python
import urllib.request

# 下载网页
url = 'http://www.baidu.com'
# url是访问的路径  filename是下载到本地之后的名字
urllib.request.urlretrieve(url=url,filename='bd.html')

# 下载图片
url = 'https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1571735604081&di=aa81344782326fc8e9f40e09aba8667f&imgtype=0&src=http%3A%2F%2Fimage.aiwenwo.net%2Fwww%2Ftvgps%2Fuploads%2Fallimg%2F160408%2F24-16040Q60305V8.jpg'
urllib.request.urlretrieve(url=url,filename='ycxWIFE.jpg')

# 下载视频
url = 'http://vd3.bdstatic.com/mda-ih0qi72n1mc0miwh/sc/mda-ih0qi72n1mc0miwh.mp4'
urllib.request.urlretrieve(url=url,filename='xiaoshipin.mp4')
```

### 4. 请求对象的定制

```python
import urllib.request

# 默认情况下 urllib.request.urlopen方法 访问服务器携带的ua是python的ua
# 那么很多网站会检测ua的真实性  如果是一个爬虫程序 那么将不会给你返回数据
url = 'http://www.baidu.com'
# 请求头
headers={
'User-Agent':'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50'
}
# urlopen方法中是没有headers的属性的 所以不可以直接在该方法中添加ua
# response = urllib.request.urlopen(url=url,headers=headers)
request = urllib.request.Request(url=url,headers=headers)
# 模拟浏览器向服务器发送请求
response = urllib.request.urlopen(request)
print(response.getcode())
```

### 5. 编解码

#### 1. get

+ get请求方式得编码(一个参数带中文name)

  ```python
  import urllib.request
  import urllib.parse
  
  # 默认情况下  浏览器会自动得进行编解码  当讲汉语复制到pycharm中得
  # 时候，那么pycharm不会自动编解码
  url = 'https://www.baidu.com/s?wd='
  name = '韩红'
  # 将name进行编码
  name = urllib.request.quote(name)
  # url和name进行拼接
  url = url + name
  # 模拟浏览器向服务器发送请求
  response = urllib.request.urlopen(url=url)
  # 读取服务器响应的数据
  t = response.read().decode('utf-8')
  print(t)
  ```

+ get请求对象的定制

  ```python
  import urllib.request
  import urllib.parse
  
  # url   data参数  headers
  url='http://www.baidu.com/s?wd='
  # 编码
  data = '1906班'
  data = urllib.parse.quote(data)
  # 拼接
  url = url + data
  
  # 请求头ua
  headers={
  'User-Agent': 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50'
  }
  
  # 请求对象的定制
  request = urllib.request.Request(url=url,headers=headers)
  # 模拟浏览器向服务器发送请求
  response = urllib.request.urlopen(request)
  # 读取服务器响应的数据
  p_source = response.read().decode('utf-8')
  print(p_source)
  ```

+ get请求多个参数编解码

  ```python
  # 多个参数执行get请求
  # http://www.baidu.com/s?name=韩美娟&sex=不知道
  
  import urllib.request
  import urllib.parse
  
  url = 'http://www.baidu.com/s?'
  # 编码
  data = {
      'wd':'韩美娟',
      'sex':'不知道',
  }
  data = urllib.parse.urlencode(data)
  # 拼接
  url = url + data
  # 请求头
  headers = {
      'User-Agent': 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50'
  }
  # 请求对象的定制
  request = urllib.request.Request(url=url,headers=headers)
  # 模拟浏览器向服务器发送请求
  response = urllib.request.urlopen(request)
  # 读取服务器响应的数据
  print(response.read().decode('utf-8'))
  ```

#### 2. post

+ post请求——一个参数

  ```python
  import urllib.request
  import urllib.parse
  
  url = 'https://fanyi.baidu.com/sug'
  # 编码
  data = {
  	'kw':'shite'
  }
  # post请求参数必须有encode('utf-8')编码
  data = urllib.parse.urlencode(data).encode('utf-8')
  # 请求头
  headers = {
  'User-Agent': 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50'
  }
  # 请求对象的定制
  request = urllib.request.Request(url=url,headers=headers,data=data)
  # 模拟浏览器向服务器发送请求
  response = urllib.request.urlopen(request)
  # 读取服务器响应的数据
  content = response.read().decode('utf-8')
  
  import json
  # 把二进制转换成中文
  obj = json.loads(content)
  s = json.dumps(obj,ensure_ascii=False)
  print(s)
  ```

+ post请求——百度翻译详细翻译

  ```python
  import urllib.request
  import urllib.parse
  
  url = 'https://fanyi.baidu.com/v2transapi?from=en&to=zh'
  # 编码
  data ={
      'from': 'en',
      'to': 'zh',
      'query': 'dream',
      'transtype': 'realtime',
      'simple_means_flag': '3',
      'sign': '679690.965691',
      'token': '1368ee2ecb7fd8616a2c928e8b0b3abb',
  }
  data = urllib.parse.urlencode(data).encode('utf-8')
  # 请求头
  headers = {
      'User-Agent': 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50'
  }
  # 请求对象的定制
  request = urllib.request.Request(url=url,headers=headers,data=data)
  # 模拟浏览器向服务器发送请求
  response = urllib.request.urlopen(request)
  # 读取服务器响应的数据
  content = response.read().decode('utf-8')
  
  import json
  # 把二进制转换成中文
  obj = json.loads(content)
  s = json.dumps(obj,ensure_ascii=False)
  print(s)
  ```

#### 3. ajax

+ post和get区别

  ```python 
  1：get请求方式的参数必须编码，参数是拼接到url后面，编码之后不需要调用encode方法
  2：post请求方式的参数必须编码，参数是放在请求对象定制的方法中，编码之后需要调用encode方法
  ```

+ ajax的get请求——豆瓣电影单页下载

  ```python
  import urllib.request
  
  url = 'https://movie.douban.com/j/chart/top_list?type=6&interval_id=100%3A90&action=&start=0&limit=20'
  # 请求头
  headers = {
  'User-Agent': 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50'
  }
  # 请求对象的定制
  request = urllib.request.Request(url=url,headers=headers)
  # 模拟浏览器向服务器发送请求
  response = urllib.request.urlopen(request)
  # 读取服务器响应的数据
  content = response.read().decode('utf-8')
  # 读取的数据写入到文件中
  with open('douban.json','w',encoding='utf-8')as fp:
  	fp.write(content)
  ```

+ ajax的get请求——豆瓣电影多页下载

  ```python
  import urllib.request
  import urllib.parse
  
  # 请求对象的定制
  def create_request(page):
      url = 'https://movie.douban.com/j/chart/top_list?'
      # 编码
      data = {
          'type': '25',
          'interval_id': '100:90',
          'action':'',
          # 1 0  2 20  3 40
          'start': (page-1)*20,
          'limit': '20'
      }
      data = urllib.parse.urlencode(data)
      # 拼接
      url = url + data
      # 请求头
      headers = {
          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
      }
      # 请求对象的定制
      request = urllib.request.Request(url=url,headers=headers)
      return request
  
  # 获取页面的源码
  def get_content(request):
      # 模拟浏览器向服务器发送请求
      response = urllib.request.urlopen(request)
      # 读取服务器响应的数据
      content = response.read().decode('utf-8')
      return content
  
  # 读取的数据写入到文件中
  def down_load(page,content):
      with open('douban_'+str(page)+'.json','w',encoding='utf-8')as fp:
          fp.write(content)
  
  if __name__ == '__main__':
      start_page = int(input('请输入起始页码'))
      end_page = int(input('请输入结束页码'))
  
      for page in range(start_page,end_page+1):
          request = create_request(page)
          content = get_content(request)
          down_load(page,content)
  ```

+ ajax的post请求——KFC官网

  ```python
  import urllib.parse
  import urllib.request
  
  # 请求对象的定制 
  def create_request(page):
      url = 'http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname'
      # 编码
      data = {
          'cname': '大连',
          'pid':'',
          'pageIndex': page,
          'pageSize': '10',
      }
      # post请求参数必须有encode('utf-8')编码
      data = urllib.parse.urlencode(data).encode('utf-8')
      # 请求头
      headers = {
          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
      }
      # 请求对象的定制
      request = urllib.request.Request(url=url,headers=headers,data=data)
      return request
  
  # 获取页面的源码
  def get_content(request):
      # 模拟浏览器向服务器发送请求
      response = urllib.request.urlopen(request)
      # 读取服务器响应的数据
      content = response.read().decode('utf-8')
      return content
  
  # 读取的数据写入到文件中
  def down_load(page,content):
      with open('kfc_'+str(page)+'.json','w',encoding='utf-8')as fp:
          fp.write(content)
  
  if __name__ == '__main__':
      start_page = int(input('请输入起始页码'))
      end_page = int(input('请输入结束页码'))
      for page in range(start_page,end_page+1):
          request = create_request(page)
          content = get_content(request)
          down_load(page,content)
  ```

### 6. 异常  HTTPError  URLError

```python
import urllib.request
import urllib.error
# 域名或者IP地址错误报URLError
# 路径错误报HTTPError
url = 'https://blog.csdn.net/ityard/article/details/102646738'
# 请求头
headers = {    
        'Cookie': 'uuid_tt_dd=10_19284691370-1530006813444-566189; smidV2=2018091619443662be2b30145de89bbb07f3f93a3167b80002b53e7acc61420; _ga=GA1.2.1823123463.1543288103; dc_session_id=10_1550457613466.265727; acw_tc=2760821d15710446036596250e10a1a7c89c3593e79928b22b3e3e2bc98b89; Hm_lvt_e5ef47b9f471504959267fd614d579cd=1571329184; Hm_ct_e5ef47b9f471504959267fd614d579cd=6525*1*10_19284691370-1530006813444-566189; __yadk_uid=r0LSXrcNYgymXooFiLaCGt1ahSCSxMCb; Hm_lvt_6bcd52f51e9b3dce32bec4a3997715ac=1571329199,1571329223,1571713144,1571799968; acw_sc__v2=5dafc3b3bc5fad549cbdea513e330fbbbee00e25; firstDie=1; SESSION=396bc85c-556b-42bd-890c-c20adaaa1e47; UserName=weixin_42565646; UserInfo=d34ab5352bfa4f21b1eb68cdacd74768; UserToken=d34ab5352bfa4f21b1eb68cdacd74768; UserNick=weixin_42565646; AU=7A5; UN=weixin_42565646; BT=1571800370777; p_uid=U000000; dc_tos=pzt4xf; Hm_lpvt_6bcd52f51e9b3dce32bec4a3997715ac=1571800372; Hm_ct_6bcd52f51e9b3dce32bec4a3997715ac=1788*1*PC_VC!6525*1*10_19284691370-1530006813444-566189!5744*1*weixin_42565646; announcement=%257B%2522isLogin%2522%253Atrue%252C%2522announcementUrl%2522%253A%2522https%253A%252F%252Fblogdev.blog.csdn.net%252Farticle%252Fdetails%252F102605809%2522%252C%2522announcementCount%2522%253A0%252C%2522announcementExpire%2522%253A3600000%257D',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36',
}

try:
    # 请求对象的定制
    request = urllib.request.Request(url=url,headers=headers)
	# 模拟浏览器向服务器发送请求
    response = urllib.request.urlopen(request)
	# 读取服务器响应的数据
    content = response.read().decode('utf-8')
    print(content)
except urllib.error.HTTPError:
    print(1111)
except urllib.error.URLError:
    print(2222)
```

### 7. cookie登录

+ 登录人人网(get)

  ```python
  import urllib.request
  import urllib.parse
  
  url = 'http://www.renren.com/305523888/profile'
  # 请求头
  headers = {
          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36',
          'Cookie': 'anonymid=jix3nuu4-498h3n; _de=BF83005E46A2ACDF72FFEFECAA50653A696BF75400CE19CC; ln_uact=595165358@qq.com; ln_hurl=http://hdn.xnimg.cn/photos/hdn521/20170509/0940/main_5crY_aee9000088781986.jpg; _r01_=1; jebe_key=b8a3f973-563c-4e6a-ac8f-99deef080f20%7Ccfcd208495d565ef66e7dff9f98764da%7C1565664618046%7C0%7C1565664616181; wp_fold=0; depovince=SH; jebecookies=dd7a7840-774a-44e8-89ab-36942629e3af|||||; JSESSIONID=abc61vSCzpF2xhumYp33w; ick_login=f9f873c2-206c-4c17-9cb9-ec4c7e3044d9; p=b178623a64102cc30833baf87c1c33f28; first_login_flag=1; t=ec055348320811e3b0b3345d15afe18c8; societyguester=ec055348320811e3b0b3345d15afe18c8; id=305523888; xnsid=8bff41c; loginfrom=syshome; jebe_key=b8a3f973-563c-4e6a-ac8f-99deef080f20%7Cdca572dcc866b00768c874af75fd79ec%7C1571811553654%7C1%7C1571811557213'
  }
  # 请求对象的对象
  request = urllib.request.Request(url=url,headers=headers)
  # 模拟浏览器向服务器发送请求
  response = urllib.request.urlopen(request)
  # 读取服务器响应的数据
  content = response.read().decode('utf-8')
  print(content)
  ```

+ 登录微博(get)

  ```python
  import urllib.request
  
  url = 'https://weibo.cn/6451491586/info'
  # 请求头
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36',
  	'cookie': 'SCF=Ahi2Sm3XHpcYIJvIsbJd8AnqkyO8t5RFmHXn8yHeTOMYgumvEqFGsgNbZbD6BmzlV7GA-B8sNWcbTcHeVmF3eNc.; _T_WM=661e0af7353e4ce48f5c9cfa8368bce5; SUB=_2A25wq4T0DeRhGeBK7lMV-S_JwzqIHXVQVyy8rDV6PUJbkdAKLXTakW1NR6e0UGDjQ3X7s7TSUa_J_IHy8f7dJeXJ; SUHB=0idbj3TaBzD_2G; SSOLoginState=1571812516',
  }
  # 请求对象的定制
  request = urllib.request.Request(url=url,headers=headers)
  # 模拟浏览器向服务器发送请求
  response = urllib.request.urlopen(request)
  # 读取服务器响应的数据
  content = response.read().decode('utf-8')
  # 读取的数据写入文件中
  with open('weibo.html','w',encoding='utf-8')as fp:
      fp.write(content)
  ```

+ 登录微博——用户名缓存(post)

  ```python
  import urllib.request
  import urllib.parse
  
  url = 'https://passport.weibo.cn/sso/login'
  # 编码
  data = {
      'username': '18642820892',
      'password': 'lijing1150501',
      'savestate': '1',
      'r': 'https://weibo.cn/',
      'ec': '0',
      'pagerefer': 'https://weibo.cn/pub/?vt=',
      'entry': 'mweibo',
      'wentry': '',
      'loginfrom': '',
      'client_id': '',
      'code': '',
      'qq': '',
      'mainpageflag': '1',
      'hff': '',
      'hfp':'',
  }
  data = urllib.parse.urlencode(data).encode('utf-8')
  # 请求头
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36',
      # 检测上一级页面
  	'Referer': 'https://passport.weibo.cn/signin/login?entry=mweibo&r=https%3A%2F%2Fweibo.cn%2F&backTitle=%CE%A2%B2%A9&vt='
  }
  # 请求对象的定制
  request = urllib.request.Request(url=url,headers=headers,data=data)
  # 模拟浏览器向服务器发送请求
  response = urllib.request.urlopen(request)
  # 读取服务器响应的数据
  content = response.read().decode('utf-8')
  print(content)
  ```


### 8. handler——定制更高级的请求头

```python
import urllib.request

url = 'http://www.baidu.com'
# 请求头
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
}
# 请求对象的定制
request = urllib.request.Request(url=url,headers=headers)

# 下面的相当于urllib.request.urlopen(request)
handler = urllib.request.HTTPHandler()
opener = urllib.request.build_opener(handler)
response = opener.open(request)
# 读取服务器响应的数据
content = response.read().decode('utf-8')
print(content)
```

### 9. IP代理

+ 西次代理——免费的

  ```python
  import urllib.request
  
  url = 'https://www.baidu.com/s?wd=ip'
  # 请求头
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
  }
  # 请求对象的定制
  request = urllib.request.Request(url=url,headers=headers)
  
  # IP代理
  proxies = {
      'https':'210.26.49.88:3128'
  }
  # 下面的相当于response = urllib.request.urlopen(request)
  handler = urllib.request.ProxyHandler(proxies=proxies)
  opener = urllib.request.build_opener(handler)
  response = opener.open(request)
  # 读取服务器响应的数据
  content = response.read().decode('utf-8')
  # 读取的数据写入文件中
  with open('daili.html','w',encoding='utf-8')as fp:
      fp.write(content)
  ```

+ 快代理——收费的

  ```python
  import urllib.request
  
  # 获取自己购买的代理IP
  url_ip='http://kps.kdlapi.com/api/getkps/?orderid=987181651846636&num=1&pt=1&sep=1'
  response = urllib.request.urlopen(url_ip)
  content_ip = response.read().decode('utf-8')
  
  url = 'https://www.baidu.com/s?wd=ip'
  # 请求头
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
  }
  # 请求对象的定制
  request = urllib.request.Request(url=url,headers=headers)
  # IP代理
  proxies = {
      'https':content_ip
  }
  # 下面的相当于response = urllib.request.urlopen(request)
  handler = urllib.request.ProxyHandler(proxies=proxies)
  opener = urllib.request.build_opener(handler)
  response = opener.open(request)
  # 读取服务器响应的数据
  content = response.read().decode('utf-8')
  # 读取的数据写入文件中
  with open('daili1.html','w',encoding='utf-8')as fp:
      fp.write(content)
  ```

+ 代理池——校花网

  ```python
  import urllib.request
  import random
  
  url = 'http://www.xiaohuar.com'
  # 请求头
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
  }
  # 请求对象的定制
  request = urllib.request.Request(url=url,headers=headers)
  # 定义了一个代理的一个列表 我们称之为代理池
  proxy = [
      {'http':'112.74.108.33:16818'},
      {'http': '112.74.108.33:16818'},
      {'http': '112.74.108.33:16818'},
  ]
  # 随机会抽取一个代理
  proxies = random.choice(proxy)
  
  # 使用hanler代理
  handler = urllib.request.ProxyHandler(proxies=proxies)
  opener = urllib.request.build_opener(handler)
  response = opener.open(request)
  # 获取响应数据
  content = response.read().decode('utf-8')
  # 保存页面
  with open('xh.html','w',encoding='utf-8')as fp:
      fp.write(content)
  ```

### 10. cookie动态获取

```python
import urllib.request
import urllib.parse
import http.cookiejar
# cookiejar  每一次登陆之后的cookie都是不一样的  那么我们如何解决登陆问题

url_post = 'http://www.quanshuwang.com/login.php?do=submit'
# 编码
data = {
    'username': 'action',
    'password': 'action',
    'action': 'login',
}
data = urllib.parse.urlencode(data).encode('gbk')
# 请求头
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
}
# 请求对象的定制
request_post = urllib.request.Request(url=url_post,data=data,headers=headers)

# 当使用了opener.open发送了请求  那么会将登陆的cookie保存到opener所在cookiejar中
cookiejar = http.cookiejar.CookieJar()
handler = urllib.request.HTTPCookieProcessor(cookiejar=cookiejar)
opener = urllib.request.build_opener(handler)
response = opener.open(request_post)

url_get = 'http://www.quanshuwang.com/modules/article/bookcase.php'
request_get = urllib.request.Request(url=url_get,headers=headers)
response = opener.open(request_get)
content = response.read().decode('gbk')
print(content)
```

## 3. 数据解析

### 1. 正则表达式

+ 单字修饰符

  ```python
  1. 匹配任意字符，除了换行符
  2. []  用来表示一组字符,单独列出：[abc] 匹配 'a'，'b'或'c'
  3. \d  匹配任意数字，等价于 [0-9].
  4. \D  匹配任意非数字
  5. \w  匹配字母数字及下划线
  6. \W  匹配非字母数字及下划线
  7. \s  匹配任意空白字符，等价于 [\t\n\r\f].
  8. \S  匹配任意非空字符
  ```

+ 数量修饰符

  ```python
  1. *    匹配0个或多个的表达式	
  2. +    匹配1个或多个的表达式
  3. ?    匹配0个或1个由前面的正则表达式定义的片段
  4. {m}  前面字符出现m次
  5. {m,} 前面字符出现至少m次
  6. {m,n}前面字符出现m~n次
  ```

+ 边界修饰符

  ```python
  1. ^    以...开始		eg:'^abc' 匹配以abc开头
  2. $    以...结尾		eg:'abc$'  匹配以abc结尾
  ```

+ 分组修饰符

  ```python
  1.() 匹配括号内的表达式，也表示一个组	
  2.\1  \2  匹配第1、2个分组的内容
  eg:（.*）:(.*)	
  	 \1    \2
  ```

+ 贪婪模式/非贪婪模式

  ```python
  贪婪模式：在整个表达式匹配成功的前提下，尽可能多的匹配 ( * )；
  非贪婪模式：在整个表达式匹配成功的前提下，尽可能少的匹配 ( ? )；
  Python里数量词默认是贪婪的。
  示例一 ： 源字符串：abbbc
  使用贪婪的数量词的正则表达式 ab* ，匹配结果： abbb。 
  * 决定了尽可能多匹配 b，所以a后面所有的 b 都出现了。
  使用非贪婪的数量词的正则表达式ab*?，匹配结果： a。 
  即使前面有 *，但是 ? 决定了尽可能少匹配 b，所以没有 b。
  
  示例二 ： 源字符串：aa<div>test1</div>bb<div>test2</div>cc
  使用贪婪的数量词的正则表达式：<div>.*</div>
  匹配结果：<div>test1</div>bb<div>test2</div>
  这里采用的是贪婪模式。在匹配到第一个“</div>”时已经可以使整个表达
  式匹配成功，但是由于采用的是贪婪模式，所以仍然要向右尝试匹配，
  查看是否还有更长的可以成功匹配的子串。匹配到第二个“</div>”后，
  向右再没有可以成功匹配的子串，匹配结束，匹配结果为
  “<div>test1</div>bb<div>test2</div>”
  
  使用非贪婪的数量词的正则表达式：<div>.*?</div>
  匹配结果：<div>test1</div>
  正则表达式二采用的是非贪婪模式，在匹配到第一个“</div>”
  时使整个表达式匹配成功，由于采用的是非贪婪模式，
  所以结束匹配，不再向右尝试，匹配结果为“<div>test1</div>”。
  ```

+ 模式修饰符

  ```python
  1. re.S  单行模式
  2. re.M  多行模式
  3. re.I 忽略大小写
  ```

### 2. 正则解析

+ 正则表达式简单使用

  ```python
  import re
  
  content = '''
      <div class="thumb">
          <a href="/article/119749308" target="_blank">
          <img src="//pic.qiushibaike.com/system/pictures/11974/119749308/medium/app119749308.jpg" alt="糗事#119749308" class="illustration" width="100%" height="auto">
          </a>
      </div>
      <div class="thumb">
          <a href="/article/119750010" target="_blank">
          <img src="//pic.qiushibaike.com/system/pictures/11975/119750010/medium/app119750010.jpg" alt="糗事#119750010" class="illustration" width="100%" height="auto">
          </a>
      </div>
      <div class="thumb">
          <a href="/article/121859652" target="_blank">
          <img src="//pic.qiushibaike.com/system/pictures/12185/121859652/medium/LZB48T44DNIAY1CV.jpg" alt="糗事#121859652" class="illustration" width="100%" height="auto">
          </a>
      </div>
  '''
  # pattern = re.compile('pic.*?\.jpg')
  # 使用正则匹配结果
  pattern = re.compile('<div class="thumb">.*?<img src="(.*?)" alt=".*?"',re.S)
  src_list = pattern.findall(content)
  for src in src_list:
      print(src)
  ```

+ 正则表达式的使用——爬取站长素材(图片)

  ```python
  import urllib.request
  import re
  
  # 请求对象的定制
  def create_request(page):
      base_url = 'http://sc.chinaz.com/tupian/qinglvtupian'
      if page == 1:
          url = base_url + '.html'
      else:
          url = base_url + '_' + str(page) + '.html'
  	# 请求头
      headers = {
          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
      }
  	# 请求对象的定制
      request = urllib.request.Request(url=url,headers=headers)
      return request
  
  # 获取页面的源码
  def get_content(request):
      # 模拟浏览器向服务器发送请求
      response = urllib.request.urlopen(request)
      # 读取服务器响应的数据
      content = response.read().decode('utf-8')
      # print(content)
      return content
  
  # 读取的数据写入到文件中
  def down_load(content):
      # pattern = re.compile('<div class="box picblock col3".*?<img src2="(.*?)" alt="(.*?)">',re.S)
      # 使用正则表达式匹配
      pattern = re.compile('<div class="box picblock col3".*?src2="(.*?)" alt="(.*?)"></a>',re.S)
      objs = pattern.findall(content)
      # 遍历
      for obj in objs:
          # 后缀名
          suffix = obj[0].split('.')[-1]
          # 拼接文件名
          filename = './qinglv/'+obj[1]+'.'+suffix
          # 下载
          urllib.request.urlretrieve(url=obj[0],filename=filename)
  
  if __name__ == '__main__':
      start_page =int(input('请输入起始页码'))
      end_page = int(input('请输入结束页码'))
      for page in range(start_page,end_page + 1):
          request = create_request(page)
          content = get_content(request)
          down_load(content)
  ```

### 3. xpath

+ xpath的使用		

  ```python
  1.安装lxml库      
  	pip install lxml 
  2.导入lxml.etree  
      from lxml import etree
  3.etree.parse()   解析本地文件
      html_tree = etree.parse('XX.html')	
  4.etree.HTML()    服务器响应文件
      html_tree = etree.HTML(response.read().decode('utf-8')	
  5.html_tree.xpath(xpath路径)
  ```

+ xpath基本语法

  ```python
  1.路径查询
  	//：查找所有子孙节点，不考虑层级关系
  	/ ：找直接子节点
  2.谓词查询
  	//div[@id]  
  	//div[@id="maincontent"]    
  3.属性查询
  	//@class         
  4.模糊查询
  	//div[contains(@id, "he")]   
  	//div[starts-with(@id, "he")] 
  5.内容查询
  	//div/h1/text()
  6.逻辑运算
  	//div[@id="head" and @class="s_down"]
  	//title | //price
  ```

### 4. xpath解析

+ xpath本地解析

  ```python
  from lxml import etree
  
  # 本地文件中必须所有的标签都有结尾
  # xpath本地解析
  tree = etree.parse('index.html')
  #       //：查找所有子孙节点，不考虑层级关系
  # 		/ ：找直接子节点
  a = tree.xpath('//div[@id="d1"]//li')
  print(a)
  
  # 找到div标签中有id属性的
  a = tree.xpath('//div[@id]')
  print(a)
  
  # 属性查询
  a = tree.xpath('//div[@id="d1"]/ul/li[1]/@class')
  print(a)
  
  a_list = tree.xpath('//div[@class="d2"]/ul/li[contains(@id,"l")]/@id')
  for a in a_list:
      print(a)
  
  a = tree.xpath('//div[@class="d2"]/ul/li[starts-with(@id,"c")]/@id')
  print(a)
  
  a = tree.xpath('//div[@id="d1"]/a/text()')
  print(a)
  # --------------下面是HTML网页代码---------------------
  <!DOCTYPE html>
  <html lang="en">
  <head>
      <meta charset="UTF-8"/>
      <title>Title</title>
  </head>
  <body>
      <div id="d1">
          <ul>
              <li class="l1">hen</li>
              <li>dog</li>
              <li>cat</li>
              <li>pig</li>
              <li>monky</li>
          </ul>
          <a href="http://www.baidu.com">zoo</a>
      </div>
      
      <div class="d2">
          <ul>
              <li id="l1">大连</li>
              <li id="l2">锦州</li>
              <li id="c1">成都</li>
              <li>大理</li>
              <li>日本</li>
          </ul>
          <a href="http://www.tuniu.com">location</a>
      </div>
  </body>
  </html>
  ```

+ xpath在线解析——百度(小标题)

  ```python
  import urllib.request
  
  url = 'http://www.baidu.com'
  # 请求头
  headers={
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
  }
  # 请求对象的定制
  request = urllib.request.Request(url=url,headers=headers)
  # 模拟浏览器向服务器发送请求
  response = urllib.request.urlopen(request)
  # 读取服务器响应的数据
  content = response.read().decode('utf-8')
  
  from lxml import etree
  # xpath线上解析
  tree = etree.HTML(content)
  a_list = tree.xpath('//div[@id="u1"]/a/text()')
  # 遍历
  for a in a_list:
      print(a)
  ```

+ xpath在线解析——站长素材(图片)

  ```python
  import urllib.request
  
  # 请求对象的定制
  def create_request(page):
      base_url = 'http://sc.chinaz.com/tupian/qinglvtupian'
      if page == 1:
          url = base_url + '.html'
      else:
          url = base_url + '_' + str(page) + '.html'
  	# 请求头
      headers = {
          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
  }
      # 请求对象的定制
      request = urllib.request.Request(url=url,headers=headers)
      return request
  
  # 获取页面的源码
  def get_content(request):
      # 模拟浏览器向服务器发送请求
      response = urllib.request.urlopen(request)
      # 读取服务器响应的数据
      content = response.read().decode('utf-8')
      return content
  
  from lxml import etree
  # 读取的数据写入到文件中
  def down_load(content):
      # xpath在线解析
      tree = etree.HTML(content)
      # 查找所需的数据
      src_list = tree.xpath('//div[@id="container"]//a/img/@src2')
      alt_list = tree.xpath('//div[@id="container"]//a/img/@alt')
      for i in range(len(src_list)):
          src = src_list[i]
          alt = alt_list[i]
          filename = './qinglv/'+alt + '.jpg'
          urllib.request.urlretrieve(url=src,filename=filename)
  
  if __name__ == '__main__':
      start_page = int(input('请输入起始页码'))
      end_page = int(input('请输入结束页码'))
  
      for page in range(start_page,end_page+1):
          request = create_request(page)
          content = get_content(request)
          down_load(content)
  ```

### 5. jsonpath

+ jsonpath的使用

  ```python
  1.安装： 
  	pip install jsonpath
  2.jsonpath的使用：
  	obj = json.load(open('json文件', 'r', encoding='utf-8'))
  	ret = jsonpath.jsonpath(obj, 'jsonpath语法')
  ```

+ jsonpath基本语言

  | Xpath | JSONPath         | Description                                                  |
  | ----- | ---------------- | ------------------------------------------------------------ |
  | /     | $                | 表示根元素                                                   |
  | .     | @                | 当前元素                                                     |
  | /     | . or []          | 子元素                                                       |
  | ..    | n/a              | 父元素                                                       |
  | //    | ..               | 递归下降，JSONPath是从E4X借鉴的。                            |
  | *     | *                | 通配符，表示所有的元素                                       |
  | @     | n/a              | 属性访问字符                                                 |
  | []    | []               | 子元素操作符                                                 |
  | \|    | [,]              | 连接操作符在XPath 结果合并其它结点集合。JSONP允许name或者数组索引。 |
  | n/a   | [start:end:step] | 数组分割操作从ES4借鉴。                                      |
  | []    | ?()              | 应用过滤表示式                                               |
  | n/a   | ()               | 脚本表达式，使用在脚本引擎下面。                             |
  | ()    | n/a              | Xpath分组                                                    |

+ json对象的转换

  ```python
  1.json.loads()	是将字符串转化为python对象
  2.json.dumps()	将python对象转化为json格式的字符串
  3.json.load()	读取json格式的文本，转化为python对象
  	json.load(open(a.json))	
  4.json.dump()	将python对象写入到文本中
  ```

### 6. jsonpath解析

+ jsonpath的基本使用

  ```python
  import jsonpath
  import json
  
  obj = json.load(open('store.json','r',encoding='utf-8'))
  
  # 书店所有书的作者
  author_list = jsonpath.jsonpath(obj,'$.store.book[*].author')
  for author in author_list:
      print(author)
  
  
  # 所有的作者
  author_list = jsonpath.jsonpath(obj,'$..author')
  for author in author_list:
      print(author)
  
  # store的所有元素。所有的books和bicycle
  a_list = jsonpath.jsonpath(obj,'$.store.*')
  
  # store里面所有东西的price
  price_list = jsonpath.jsonpath(obj,'$.store..price')
  	for price in price_list:
       	print(price)
  
  #
  # 第三个书
  book = jsonpath.jsonpath(obj,'$..book[2]')
  print(book)
  
  # 最后一本书
  book = jsonpath.jsonpath(obj,'$..book[(@.length-1)]')
  print(book)
  
  # 	前面的两本书。
  book_list = jsonpath.jsonpath(obj,'$..book[0,1]')
  print(book_list)
  book_list = jsonpath.jsonpath(obj,'$..book[:2]')
  print(book_list)
  
  # 过滤出所有的包含isbn的书。
  book_list = jsonpath.jsonpath(obj,'$..book[?(@.isbn)]')
  print(book_list)
  
  # 	过滤出价格低于10的书
  book_list = jsonpath.jsonpath(obj,'$..book[?(@.price < 10)]')
  print(book_list)
  
  # ---------------------以下是要JSON数据-----------------
  { "store": {
      "book": [
        { "category": "玄幻",
          "author": "天蚕土豆",
          "title": "斗破苍穹",
          "price": 8.95
        },
        { "category": "玄幻",
          "author": "唐家三少",
          "title": "斗罗大陆",
          "price": 12.99
        },
        { "category": "言情",
          "author": "我吃西红柿",
          "title": "星辰变",
          "isbn": "0-553-21311-3",
          "price": 8.99
        },
        { "category": "言情",
          "author": "吉多",
          "title": "python从入门到放弃",
          "isbn": "0-395-19395-8",
          "price": 22.99
        }
      ],
      "bicycle": {
        "color": "red",
        "price": 19.95
      }
    }
  }
  ```

+ jsonpath解析——智联招聘

  ```python
  import jsonpath
  import json
  import urllib.request
  
  # API接口
  url = 'https://fe-api.zhaopin.com/c/i/sou?pageSize=90&cityId=538&workExperience=-1&education=-1&companyType=-1&employmentType=-1&jobWelfareTag=-1&kw=python&kt=3&_v=0.45173956&x-zp-page-request-id=8e8e88b85c494aac8ab3533ebc6ddd5a-1571908246825-8289&x-zp-client-id=94df40c0-69a5-40f2-9d66-0bbe28f523a4'
  # 请求头
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
  }
  # 请求对象的定制
  request = urllib.request.Request(url=url,headers=headers)
  # 模拟浏览器向服务器发送请求 
  response = urllib.request.urlopen(request)
  # 读取服务器响应的数据
  content = response.read().decode('utf-8')
  # 读取的数据写入到文件中
  with open('zhilian.json','w',encoding='utf-8')as fp:
      fp.write(content)
  # jsonpath本地解析(读取从网页保存到本地的数据)
  obj = json.load(open('zhilian.json','r',encoding='utf-8'))
  # 解析自己所需的数据
  jobname_list = jsonpath.jsonpath(obj,'$..jobName')
  companyname_list = jsonpath.jsonpath(obj,'$..company.name')
  salary_list = jsonpath.jsonpath(obj,'$..salary')
  
  jobInfo_list = []
  
  for i in range(len(jobname_list)):
      job = jobname_list[i]
      cname = companyname_list[i]
      salary = salary_list[i]
      jobInfo = {}
      jobInfo['job']=job
      jobInfo['cname']=cname
      jobInfo['salary']=salary
      jobInfo_list.append(jobInfo)
  # 保存到本地文件中
  with open('zhilian1.json','w',encoding='utf-8')as fp:
      fp.write(str(jobInfo_list))
  ```


### 7. BeautifulSoup

+ BeautifulSoup基本简介——bs4

  ```python
  1.BeautifulSoup简称：
  	bs4
  2.什么是BeatifulSoup？
  	BeautifulSoup，和lxml一样，是一个html的解析器，主要功能也是解析和提取数据
  3.优缺点？
  	缺点：效率没有lxml的效率高	
  	优点：接口设计人性化，使用方便
  ```

+ BeautifulSoup安装和创建

  ```python
  1.安装
  	pip install bs4
  2.导入
  	from bs4 import BeautifulSoup
  3.创建对象
  	1.本地文件生成对象
  		soup = BeautifulSoup(open('1.html'), 'lxml')
  		注意：默认打开文件的编码格式gbk所以需要指定打开编码格式	
  	2.服务器响应的文件生成对象
  		soup = BeautifulSoup(response.read().decode(), 'lxml')
  ```

+ BeautifulSoup基础知识——节点定位

  ```python
  1.根据标签名查找节点
  	soup.a 【注】只能找到第一个a
  		soup.a.name
  		soup.a.attrs
  2.函数
  	1.find(返回一个对象)
      	find('a')：只找到第一个a标签
          find('a', title='名字')
          find('a', class_='名字')
  	2.find_all(返回一个列表)
      	find_all('a')  查找到所有的a
          find_all(['a', 'span'])  返回所有的a和span
          find_all('a', limit=2)  只找前两个a
  	3.select(根据选择器得到节点对象)【推荐】
          1.element
          	eg:p
          2..class
              eg:.firstname
          3.#id
              eg:#firstname
          4.属性选择器
              [attribute]
              	eg:li = soup.select('li[class]')
              [attribute=value]
              	eg:li = soup.select('li[class="hengheng1"]')
          5.层级选择器
              1.后代选择器div p
              2.子代选择器div>p
              3.群组选择器div,p 
              	eg:soup = soup.select('a,span')
  3.获取子孙节点
  	contents：返回的是一个列表
  		eg:print(soup.body.contents)
      descendants：返回的是一个生成器
     		eg:for a in soup.body.descendants:
          	print(a)
  ```

+ BeautifulSoup基础知识——节点信息

  ```
  1.获取节点内容：适用于标签中嵌套标签的结构
  	obj.string
  	obj.get_text()【推荐】
  2.节点的属性
  	tag.name 获取标签名
  		eg: tag = find('li)
  			print(tag.name)
  	tag.attrs将属性值作为一个字典返回
  (3).获取节点属性
  	obj.attrs.get('title')【常用】
  	obj.get('title')
  	obj['title']
  ```

+ BeautifulSoup基础知识——节点类型（理解）

  ```python
  bs4.BeautifulSoup 根节点类型
  bs4.element.NavigableString 连接类型  可执行的字符串
  bs4.element.Tag 节点类型  
  bs4.element.Comment 注释类型
  	eg:
  		if type(aobj.string) == bs4.element.Comment:
          	print('这个是注释内容')
          else:
              print('这不是注释')
  ```

### 8. BeautifulSoup解析

+ bs4的基本语法使用

  ```python
  from bs4 import BeautifulSoup
  
  # 加载的是本地文件  bs4默认打开文件的编码是gbk
  soup = BeautifulSoup(open('index.html',encoding='utf-8'),'lxml')
  # 根据标签名查找节点只能找到第一个标签
  print(soup.a)
  
  # find 方法返回的是一个对象
  # 查找到第一个a标签
  a = soup.find('a')
  print(a)
  # 根据标签的属性title来查找标签
  a = soup.find('a',title='qf')
  print(a)
  # 不可以实现
  # a = soup.find('a',name='fq')
  # print(a)
  # 根据标签的属性id来查找标签
  a = soup.find('a',id='qf1')
  print(a)
  # 根据标签的属性class_来查找标签--class是关键字所以要加下划线
  a = soup.find('a',class_='bd')
  print(a)
  # 根据标签的自定义的属性bd来查找标签
  a = soup.find('a',bd='bd1')
  print(a)
  
  # findall方法返回一个列表
  # 返回所有的a标签
  a_list = soup.find_all('a')
  print(a_list)
  # 返回所有的a标签和span标签
  a_list = soup.find_all(['a','span'])
  print(a_list)
  # 返回前两个li标签
  li_list = soup.find_all('li',limit=2)
  print(li_list)
  
  # select 根据选择器得到节点对象
  # 通过标签选择器a来获取a标签
  a = soup.select('a')
  print(a)
  # 通过类选择器来获取li标签
  li = soup.select('.l1')
  print(li)
  # 通过id选择器来获取
  li = soup.select('#l2')
  print(li)
  # 通过标签属性选择器来获取
  li = soup.select('li[class]')
  print(li)
  # 通过标签属性选择器来获取
  li = soup.select('li[class="l1"]')
  print(li)
  # 通过后代选择器来获取
  li_list = soup.select('div li')
  print(li_list)
  # 通过后代选择器来获取(注意：空格)
  li = soup.select('span > span')
  print(li)
  # 通过群组选择器获取a或者span标签
  list = soup.select('a,span')
  print(list)
  
  # 获取某标签下的所有的子节点
  print(soup.body.contents)
  # 遍历(以生成器形式遍历)
  for a in soup.body.descendants:
     print(a)
  
  # 获取节点内容
  s = soup.select('#s1')[0]
  print(type(s))
  # <span id="s1">
  #         <span>还有一周我们爬虫就结束了</span>
  # </span>
  # string想要获取 那么该对象的标签内 不允许在存在标签
  print(s.string)
  # get_text()获取标签内的文本
  print(s.get_text())
  
  # name属性获取的是标签的名字
  s = soup.select('#s1')[0]
  print(s.name)
  
  # 获取节点的属性
  p = soup.select('#p1')[0]
  print(p.attrs)
  print(p.attrs.get('class'))
  print(p.get('name'))
  print(p['id'])
  # -------------------下面是HTML------------------
  <!DOCTYPE html>
  <html lang="en">
  <head>
      <meta charset="UTF-8">
      <title>Title</title>
  </head>
  <body>
      <p id="p1" class="p2" name="p3"></p>
      <span id="s1">
          <span>还有一周我们爬虫就结束了</span>
          我们就再见了
      </span>
      <div>
          <ul>
              <li class="l1">酸菜炖排骨</li>
              <li id="l2">干豆腐卷大葱</li>
              <li>宁波再回头牛蛙</li>
              <li>周黑鸭+啤酒</li>
              <li>炒鸡蛋</li>
          </ul>
          <a href="http://www.baidu.com" class="bd" bd="bd1">百度</a>
      </div>
      <div>
          <ul>
              <li>桂林</li>
              <li>马尔代夫</li>
              <li>土耳其</li>
              <li>张家堡村</li>
              <li>大连</li>
          </ul>
          <span>当当</span>
          <a href="http://www.1000phone.com" title="qf" name="fq" id="qf1">千锋</a>
      </div>
  </body>
  </html>
  ```

+ bs4解析——股票网

  ```python
  import urllib.request
  from bs4 import BeautifulSoup
  # 请求对象定制
  def create_request():
      # url
      url = 'http://quote.stockstar.com/'
      # 请求头
      headers = {
          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
      }
      # 请求对象定制
      request = urllib.request.Request(url=url,headers=headers)
      return request
  
  # 获取服务器响应的数据
  def get_content(request):
      # 模拟浏览器向服务器发送数据
      response = urllib.request.urlopen(request)
      # 获取服务器响应的数据
      content = response.read().decode('gb2312')
      # print(content)
      return content
  
  # 把获取的数据保存到文件中
  def down_load(content):
      # 用bs4解析
      content = BeautifulSoup(content,'lxml')
      # 通过解析获取自己想要的数据
      num_name_list = content.select('#datalist a')
      price_list = content.select('#datalist span')
      data = {}
      for i in range(int(len(num_name_list)/2)):
          data['num'] = num_name_list[2*i].get_text()
          data['name'] = num_name_list[2*i+1].get_text()
          data['price'] = price_list[2*i].get_text()
          print(data)
  
  if __name__ == '__main__':
      request = create_request()
      content = get_content(request)
      down_load(content)
  ```


## 4. selenium

### 1. selenium 安装

```python
1.操作谷歌浏览器驱动下载地址
	http://chromedriver.storage.googleapis.com/index.html 
2.谷歌驱动和谷歌浏览器版本之间的映射表
	http://blog.csdn.net/huilan_same/article/details/51896672
3.查看谷歌浏览器版本
	谷歌浏览器右上角-->帮助-->关于
4.安装selenium
	pip install selenium
```

### 2. selenium的使用

+ selenium的知识点

  ```python
  1.导包：
  	from selenium import webdriver
  2.创建谷歌浏览器操作对象：
  	path = 谷歌浏览器驱动文件路径
  	browser = webdriver.Chrome(path)
  3.访问网址
  	url = 要访问的网址
  	browser.get(url)
  ```

+ selenium的基本使用

  ```python
  from selenium import webdriver
  import time
  # 谷歌浏览器驱动文件的路径
  path = 'chromedriver.exe'
  # 创建谷歌浏览器操作对象
  browser = webdriver.Chrome(path)
  # 访问的网址(访问的路由)
  url = 'http://www.baidu.com'
  # 访问浏览器
  browser.get(url=url)
  # 延迟3s
  time.sleep(3)
  # 退出浏览器
  browser.quit()
  ```

### 3.  selenium元素定位与交互

+ selenium元素的定位

  ```python
  1.find_element_by_id			# 通过id来定位元素
  2.find_elements_by_name			# 通过class属性来定位元素
  3.find_elements_by_xpath		# 通过xpath来定位元素
  4.find_elements_by_tag_name 	# 通过标签来定位元素
  5.find_elements_by_css_selector	# 通过bs4来定位元素
  6.find_elements_by_link_text	# 通过a标签的文本来定位元素
  ```

+ 访问元素信息

  ```python
  .get_attribute('class')		# 获取元素属性
  .text						# 获取元素文本
  .id							# 获取id
  .tag_name					# 获取标签名
  ```

+ 交互

  ```python
  点击:click()
  输入:send_keys()
  后退操作:browser.back()
  前进操作:browser.forword()
  模拟JS滚动:
  	js = 'document.body.scrollTop=100000' # 或则下面一句
  	js='document.documentElement.scrollTop=100000'
  	browser.execute_script(js) 执行js代码
  获取网页代码：page_source 
  退出：browser.quit()
  ```

+ selenium元素定位与交互的基本使用

  ```python
  from selenium import webdriver
  import time
  # 谷歌浏览器驱动文件的路径
  path = 'chromedriver.exe'
  # 创建谷歌浏览器操作对象
  browser = webdriver.Chrome(path)
  # 访问的网址(访问的路由)
  url = 'http://www.baidu.com'
  # 访问浏览器
  browser.get(url=url)
  # 延迟2s
  time.sleep(2)
  # 通过id来定位input这个标签
  t = browser.find_element_by_id('kw')
  # 向里面输入'韩红'
  t.send_keys('韩红')
  # 延迟3s
  time.sleep(3)
  # 通过id来定位'百度一下'标签
  su = browser.find_element_by_id('su')
  # 点击
  su.click()
  # 延迟3s
  time.sleep(3)
  # 返回上一个页面
  browser.back()
  # 延迟3s
  time.sleep(3)
  # 向前操作一步
  browser.forward()
  # 延迟2s
  time.sleep(2)
  # 退出
  browser.quit()
  ```

### 5. Phantomjs

+ phantomjs的s使用步骤

  ```python
  1.获取PhantomJS.exe文件路径path
  2.browser = webdriver.PhantomJS(path)
  3.browser.get(url)
  扩展：保存屏幕快照:browser.save_screenshot('baidu.png')
  ```

+ phantomjs的基本使用

  ```python
  from selenium import webdriver
  import time
  # 谷歌浏览器驱动文件的路径
  path = 'phantomjs.exe'
  # 创建谷歌浏览器操作对象
  browser = webdriver.PhantomJS(path)
  # 访问的网址(访问的路由)
  url = 'http://www.baidu.com'
  # 访问浏览器
  browser.get(url=url)
  # 保存屏幕快照
  browser.save_screenshot('baidu.png')
  # 延迟2s
  time.sleep(2)
  # 通过id来定位input这个标签
  t = browser.find_element_by_id('kw')
  # 向里面输入'韩红'
  t.send_keys('韩红')
  # 保存屏幕快照
  browser.save_screenshot('baidu1.png')
  # 通过id来定位'百度一下'标签
  b = browser.find_element_by_id('su')
  # 点击
  b.click()
  # 保存屏幕快照
  browser.save_screenshot('baidu2.png')
  # 向下滑动到底
  js='document.documentElement.scrollTop=100000'
  # 执行js代码
  browser.execute_script(js)
  # 保存屏幕快照
  browser.save_screenshot('baidu3.png')
  ```

### 6. Chrome handless

+ 配置

  ```python
  from selenium import webdriver
  from selenium.webdriver.chrome.options import Options
  
  chrome_options = Options()
  chrome_options.add_argument('--headless')
  chrome_options.add_argument('--disable-gpu')
  
  # google浏览器的路径
  path = r'C:\Program Files (x86)\Google\Chrome\Application\chrome.exe'
  chrome_options.binary_location = path
  
  browser = webdriver.Chrome(chrome_options=chrome_options)
  browser.get('http://www.baidu.com/')
  browser.save_screenshot('baidu.png')
  ```

+ 配置封装

  ```python
  from selenium import webdriver
  # 这个是浏览器自带的  不需要我们再做额外的操作
  from selenium.webdriver.chrome.options import Options
  
  def share_browser():
      # 初始化
      chrome_options = Options()
      chrome_options.add_argument('--headless')
      chrome_options.add_argument('--disable-gpu')
      # 浏览器的安装路径    打开文件位置
      # 这个路径是你谷歌浏览器的路径
      path = r'C:\Program Files (x86)\Google\Chrome\Application\chrome.exe'
      chrome_options.binary_location = path
      browser = webdriver.Chrome(chrome_options=chrome_options)
      return browser
  # ----------------以下是封装调用-------------------------
  from handless import share_browser
  
  browser = share_browser()
  browser.get('http://www.baidu.com/')
  browser.save_screenshot('handless1.png')
  ```

## 5. request

### 1. request知识点

```
1.文档
	官方文档:http://cn.python-requests.org/zh_CN/latest/
	快速上手:http://cn.python-requests.org/zh_CN/latest/user/quickstart.html
2.安装
	pip install request
```

### 2. get请求

+ get请求知识点

  ```python
  定制参数
  	参数使用params传递
  	参数无需urlencode
  1. r.text : 获取网站源码
  2. r.encoding 访问或定制编码方式
  3. r.url 获取请求的url
  4. r.content 响应的字节类型
  5. r.status_code 响应的状态码
  6. r.headers 响应的头信息
  ```

+ get请求的基本使用

  ```python
  import requests
  # 访问的网址(访问的路由)
  url = 'http://www.baidu.com'
  # 访问浏览器
  response = requests.get(url=url)
  # 查看response的类型
  print(type(response))
  # 以二进制格式来读取文件
  print(response.content)
  # 以字符串格式来读取文件
  print(response.text)
  # 获取服务器响应的状态码
  print(response.status_code)
  # 获取请求的url
  print(response.url)
  # 获取响应的头信息
  print(response.headers)
  # 响应的时候设置编码  打印response.text的时候  那么有乱码发生 那么我们
  # 可以使用response.encoding来设置编码
  response.encoding = 'utf-8'
  ```

+ get的例子

  ```python
  import requests
  # get请求的？可以加也可以不加
  url = 'https://www.baidu.com/s'
  # 参数
  data = {
      'wd':'陈冠希'
  }
  # 请求头
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36'
  }
  # 访问浏览器
  response = requests.get(url=url,params=data,headers=headers)
  # 获取网站源码
  content = response.text
  print(content)
  ```

### 3. post请求

+ post请求的例子

  ```python
  import requests
  # 要访问的网址
  url = 'https://fanyi.baidu.com/sug'
  # 参数
  data = {
      'kw': 'abandon'
  }
  # 请求头
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36'
  }
  # 访问浏览器
  response = requests.post(url=url,data=data,headers=headers)
  # 获取网站源码
  content = response.text
  
  # 解决中文乱码
  import json
  # 将字符串转换成json对象
  obj = json.loads(content)
  # 将json对象转换成字符串，并对其编码
  s = json.dumps(obj,ensure_ascii=False)
  print(s)
  ```

+ get  和post区别

  ```
  1：get请求的参数名字是params  post请求的参数的名字是data  
  2 请求资源路径后面可以不加?  
  3 不需要手动编解码  4 不需要做请求对象的定制
  ```

### 4. proxy定制

```python
import requests
# 要访问的网址
url = 'http://www.baidu.com/s'
# 参数
data = {
    'wd':'ip'
}
# 请求头
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36'
}
# IP代理
proxies = {
    'http':'114.239.147.135:9999'
}
# 访问浏览器
response = requests.get(url=url,params=data,headers=headers,proxies=proxies)
# 获取网站源码
content = response.text
# 保存到本地文件中
with open('dl.html','w',encoding='utf-8')as fp:
    fp.write(content)
```

### 5. cookie定制

+ 笑话集

  ```python
  import requests
  # 要访问的网址
  url_get = 'http://www.jokeji.cn/user/c.asp'
  # 参数
  data = {
      'u': 'action',
      'p': 'action123',
      'sn': '1',
      't': 'big',
  }
  # 请求头
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36'
  }
  # 设置session
  session = requests.session()
  # 当通过session去访问的时候 那么会将响应的所有的数据绑定再session上
  session.get(url=url_get,params=data,headers=headers)
  # 要访问的网址
  url = 'http://www.jokeji.cn/User/MemberCenter.asp'
  # 再次通过session去访问 那么将会将之前返回的信息提交给该请求
  response = session.get(url=url,headers=headers)
  # requests默认的编码格式是iso-8859-1
  response.encoding = 'gb2312'
  # 获取网站源码
  content = response.text
  # 保存到本地文件中
  with open('xh.html','w',encoding='gb2312')as fp:
      fp.write(content)
  ```

+ 全书网

  ```python
  import requests
  # 要访问的网址
  url = 'http://www.quanshuwang.com/login.php?do=submit'
  # 参数
  data = {
      'username': 'action',
      'password': 'action',
      'action': 'login',
  }
  # 请求头
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36'
  }
  # 设置session
  session = requests.session()
  # 当通过session去访问的时候 那么会将响应的所有的数据绑定再session上
  session.post(url=url,data=data,headers=headers)
  # 要访问的网址
  url_get = 'http://www.quanshuwang.com/modules/article/bookcase.php'
  # 再次通过session去访问 那么将会将之前返回的信息提交给该请求
  response = session.get(url=url_get,headers=headers)
  # 编码
  response.encoding = 'gbk'
  # 获取网站源码 
  content = response.text
  # 保存到本地文件中
  with open('qs.html','w',encoding='gbk')as fp:
      fp.write(content)
  ```

+ 古诗文网

  ```python
  import requests
  from bs4 import BeautifulSoup
  import urllib.request
  # 要访问的网址
  url = 'https://so.gushiwen.org/user/login.aspx?from=http://so.gushiwen.org/user/collect.aspx'
  # 请求头
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36'
  }
  # 设置session
  session = requests.session()
  # 访问浏览器
  response = requests.get(url=url,headers=headers)
  # 登陆页面的源码
  content = response.text
  # 用bs4解析
  soup = BeautifulSoup(content,'lxml')
  # 解析自己想要的数据
  viewstate = soup.select('#__VIEWSTATE')[0].attrs.get('value')
  viewstategenerator = soup.select('#__VIEWSTATEGENERATOR')[0].attrs.get('value')
  # 获取img
  code = soup.select('#imgCode')[0].attrs.get('src')
  # 拼接
  code_url = 'https://so.gushiwen.org' + code
  # 
  response_code = session.get(url=code_url)
  content_code=response_code.content
  with open('code.jpg','wb')as fp:
      fp.write(content_code)
  
  codename = input('请输入验证码')
  # 提交表单
  url_post = 'https://so.gushiwen.org/user/login.aspx?from=http%3a%2f%2fso.gushiwen.org%2fuser%2fcollect.aspx'
  data_post = {
      '__VIEWSTATE': viewstate,
      '__VIEWSTATEGENERATOR': viewstategenerator,
      'from': 'http://so.gushiwen.org/user/collect.aspx',
      'email': '595165358@qq.com',
      'pwd': 'action',
      'code': codename,
      'denglu': '登录',
  }
  response_post = session.post(url=url,headers=headers,data=data_post)
  content_post = response_post.text
  with open('gushiwen.html','w',encoding='utf-8')as fp:
      fp.write(content_post)
  ```


## 6. scrapy

### 1. scrapy基本知识

```python
1.scrapy概念
	Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。
2.安装scrapy
	1.pip install wheel
	2.pip install Twisted
	3.pip install lxml
	4.pip install pypiwin32
	5.pip install scrapy
3.scrapy项目创建
	1.scrapy startproject  项目名称
4.项目组成
	spiders 
    	__init__.py
        自定义的爬虫文件.py	===>由我们自己创建，是实现爬虫核心功能的文件
    __init__.py                  
    items.py             ===>定义数据结构的地方，是一个继承自scrapy.Item的类
    middlewares.py       ===>中间件   代理
    pipelines.py		 ===>管道文件，里面只有一个类，用于处理下载数据的后续处理
		默认是300优先级，值越小优先级越高（1-1000）
    settings.py			 ===>配置文件  比如：遵守robots协议，User-Agent定义
5.创建爬虫文件
	1.跳转到spiders文件夹  cd 目录名字/目录名字/spiders
	2.scrapy genspider 爬虫名字 网页的域名
	3.爬虫文件的组成
		1.name = 'bd'# 爬虫的文件的名字
		2.allowed_domains = ['http://www.baidu.com']# 允许访问的域名
		3.start_urls = ['http://www.baidu.com/']# 起始的url
		4.def parse(self, response)
        	1.response.text 读取网页的
            2.response.body 以二进制形式读取网页
            3.response.xpath() 用xpath解析网页
            	1.extract()提取的是selector对象的是data
                2.extract_first()提取的是selector列表中的第一个数据
6.运行爬虫文件
	scrapy crawl 爬虫名称 # 注意：应在spiders文件夹内执行
7.扩展：导出文件
	-o name.json
	-o name.xml
	-o name.csv
```

### 2. scrapy工作原理

![](G:\学习资料\笔记\Typora\总结笔记\img\scrapy原理.png)

![](G:\学习资料\笔记\Typora\总结笔记\img\scrapy原理_英文.png)

### 3. scrapy shell

```python
1.安装ipython
	pip install ipython
2.应用：1.scrapy shell www.baidu.com
       2.scrapy shell http://www.baidu.com
       3.scrapy shell "http://www.baidu.com"
       4.scrapy shell "www.baidu.com"
3.语法
	1.response对象
		1.response.body	# 以二进制形式读取网页
        2.response.text	# 读取网页
        3.response.url		# 请求路由
        4.response.status	# 状态码
	2.response的解析
    	1.response.xpath() 使用xpath路径查询特定元素，返回一个selector列表对象
        2.response.css()使用css_selector查询元素，返回一个selector列表对象
        	获取内容 ：response.css('#su::text').extract_first()
            获取属性 ：response.css('#su::attr(“value”)').extract_first()
	3.selector对象（通过xpath方法调用返回的是seletor列表）
    	1.extract()提取的是selector对象的是data
        2.extract_first()提取的是selector列表中的第一个数据
    4.item对象
    	1.dict(itemobj)可以使用dict方法直接将item对象转换成字典对象
        2.Item(dicobj) 可以使用字典对象创建一个Item对象
```

## 7. yield

### 1. yield基础与例子

```python
1.带有yield的函数不再是一个普通函数，而是一个生成器generator，可用于迭代
2.yield 是一个类似return的关键字，迭代一次遇到yield时就返回yield后面(右边)的值。重点是：下一次迭代时，从上一次迭代遇到的yield后面的代码(下一行)开始执行
3.简要理解：yield就是return返回一个值，并且记住这个返回的位置，下次迭代就从这个位置后(下一行)开始
```

```python
name_list = [x for x in range(10)]
def createGenorator():
	items = []
	for i in name_list:
		print('第{}次调用'.format(i))
		items.append(i)
	return items
def testFunc1():
	generator = createGenorator2()
	for a in generator:
		print('使用第{}次'.format(a))
def createGenorator2():
	for i in name_list:
		print('第{}次调用'.format(i))
		yield i
print(testFunc1())
```

### 2. scrapy爬取图片——站长素材(多管道多页下载)

```python
##################自定义的爬虫文件.py################
# -*- coding: utf-8 -*-
import scrapy
from ..items import ZhanzhangItem

class ZzSpider(scrapy.Spider):
    name = 'zz'	# 爬虫文件名字
    allowed_domains = ['sc.chinaz.com']		# 爬取网页的根路由
    start_urls = ['http://sc.chinaz.com/tupian/shanshuifengjing.html']# 开始爬取的网页
    page = 1 
    base_url = 'http://sc.chinaz.com/tupian/shanshuifengjing_'
    def parse(self, response):
        img_list = response.xpath('//div[@id="container"]//a/img')
        for img in img_list:
            src = img.xpath('./@src2').extract_first()
            alt = img.xpath('./@alt').extract_first()
            zz = ZhanzhangItem(src=src,alt=alt)
            yield zz
		# 把前5页的url 找到
        if self.page <= 5:
            self.page = self.page + 1
            url = self.base_url + str(self.page) + '.html'
            print('=========================')
            print(url)
            # callback的值 不加（）
            yield scrapy.Request(url=url,callback=self.parse)
####################items.py########################
# -*- coding: utf-8 -*-
import scrapy
class ZhanzhangItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    # 定义数据结构
    src = scrapy.Field()
    alt = scrapy.Field()
###################pipelines.py##################
# -*- coding: utf-8 -*-
import urllib.request
# 写入zz.json中
class ZhanzhangPipeline(object):
	# 爬虫开始执行的
    def open_spider(self,spider):
        self.fp = open('zz.json','w',encoding='utf-8')
    # 爬虫执行时执行的
    def process_item(self, item, spider):
        self.fp.write(str(item))
        return item
	# 爬虫结束执行的
    def close_spider(self,spdier):
        self.fp.close()
# 下载图片
class ZhanzhangDownLoadPipeline(object):
    def process_item(self,item,spider):
        src = item['src']
        alt = item['alt']
        filename = './mw/'+alt+'.jpg'
        urllib.request.urlretrieve(url=src,filename=filename)
        return item
######################settings.py####################
ITEM_PIPELINES = {
   'zhanzhang.pipelines.ZhanzhangPipeline': 300,
   'zhanzhang.pipelines.ZhanzhangDownLoadPipeline':250,
}
```

### 3. scrapy爬取一级界面的标题和二级界面的图片——电影天堂

```python
##############自定义的爬虫文件.py#################
# -*- coding: utf-8 -*-
import scrapy
from ..items import MovieItem

class MvSpider(scrapy.Spider):
    name = 'mv'
    allowed_domains = ['www.ygdy8.net']
    start_urls = ['https://www.ygdy8.net/html/gndy/dyzz/index.html']

    def parse(self, response):
        a_list = response.xpath('//div[@class="co_content8"]//b/a')
        for a in a_list:
            name = a.xpath('./text()').extract_first()
            href = a.xpath('./@href').extract_first()
            url = 'https://www.ygdy8.net' + href
            yield scrapy.Request(url=url,callback=self.parse_second,meta={'name':name})

    def parse_second(self,response):
        name = response.meta['name']
        src = response.xpath('//div[@id="Zoom"]//img[1]/@src').extract_first()
        movie = MovieItem(name=name,src=src)
        yield movie
###################items.py###########################
# -*- coding: utf-8 -*-
import scrapy

class MovieItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    # 定义数据结构
    name = scrapy.Field()
    src = scrapy.Field()
###################pipelines.py#########################
class MoviePipeline(object):
    def open_spider(self,spider):
        self.fp = open('movie.json','w',encoding='utf-8')
    def process_item(self, item, spider):
        self.fp.write(str(item))
        return item
    def close_spider(self,spider):
        self.fp.close()
```

### 4. MySQL

```
1.下载
	https://dev.mysql.com/downloads/windows/installer/5.7.html
2.安装
	https://jingyan.baidu.com/album/d7130635f1c77d13fdf475df.html
3.pymysql的使用
	1.pip install pymysql
	2.conn = pymysql.Connect(host,port,user,password,db,charset)
	3.conn.cursor()
	4.cursor.execute(sql)
	5.conn.commit()
    6.cursor.close()
    7.conn.close()
```

## 8. CrawlSpider—跟进提取链接

### 1. CrawlSpider基础知识

```
1.继承自scrapy.Spider
2.独门秘笈
	CrawlSpider可以定义规则，再解析html内容的时候，可以根据链接规则提取出指定的链接，然后再向这些链接发送请求。所以，如果有需要跟进链接的需求，意思就是爬取了网页之后，需要提取链接再次爬取，使用CrawlSpider是非常合适的
3.提取链接
	链接提取器，在这里就可以写规则提取指定链接
	scrapy.linkextractors.LinkExtractor(
	 	allow = (),           # 正则表达式  提取符合正则的链接
	 	deny = (),            # (不用)正则表达式  不提取符合正则的链接
	 	allow_domains = (),   # （不用）允许的域名
		deny_domains = (),    # （不用）不允许的域名
	 	restrict_xpaths = (), # xpath，提取符合xpath规则的链接
	 	restrict_css = ()     # 提取符合选择器规则的链接)
4.模拟使用
		正则用法：links1 = LinkExtractor(allow=r'list_23_\d+\.html')
		xpath用法：links2 = LinkExtractor(restrict_xpaths=r'//div[@class="x"]')
		css用法：links3 = LinkExtractor(restrict_css='.x')
5.提取连接
		link.extract_links(response)
################需求：读书网数据入库###############
6.创建项目：scrapy startproject dushuproject
7.跳转到spiders路径  cd\dushuproject\dushuproject\spiders
8.创建爬虫类：scrapy genspider -t crawl read www.dushu.com
9.items
10.spiders
11.settings
12.pipelines
		数据保存到本地
		数据保存到mysql数据库
```

### 2. CrawlSpider跟进提取链接——读书网

```python
################自定义的爬虫文件.py######################
# -*- coding: utf-8 -*-
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from ..items import ReadItem

class ReadbookSpider(CrawlSpider):
    name = 'readbook'
    allowed_domains = ['www.dushu.com']
    start_urls = ['https://www.dushu.com/book/1107_1.html']

    # 会提取出当前页面所有符合规则连接
    rules = (
        Rule(LinkExtractor(allow=r'/book/1107_\d+.html'),
                           callback='parse_item',
                           follow=True),
    )

    def parse_item(self, response):
        img_list = response.xpath('//div[@class="bookslist"]//img')
        for img in img_list:
            src = img.xpath('./@data-original').extract_first()
            name = img.xpath('./@alt').extract_first()
            book = ReadItem(src=src,name=name)
            yield book
###############items.py###########################
# -*- coding: utf-8 -*-
import pymysql
from scrapy.utils.project import get_project_settings
import scrapy
class ReadItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    src = scrapy.Field()
    name = scrapy.Field()
##################pipelines.py#########################
# -*- coding: utf-8 -*-
class ReadPipeline(object):
    def open_spider(self,spider):
        self.fp = open('book.json','w',encoding='utf-8')
    def process_item(self, item, spider):
        self.fp.write(str(item))
      	return item
    def close_spider(self,spider):
        self.fp.close()

class ReadMysqlPipeline(object):
    def open_spider(self,spider):
    	# 建立数据库的连接
        self.conn = self.getconn()
        self.cursor = self.conn.cursor()
    def getconn(self):
        settings = get_project_settings()
        conn = pymysql.Connect(host=settings['DB_HOST'],
                        user=settings['DB_USER'],
                        password=settings['DB_PASSWORD'],
                        database=settings['DB_DATABASE'],
                        # 端口号是整数
                        port=settings['DB_PORT'],
                        # charset=utf8 没有-
                        charset=settings['DB_CHARSET'])
        return conn
    def process_item(self,item,spider):
        sql = 'insert into book1905 values ("{}","{}")'.format(item['src'],item['name'])
        self.cursor.execute(sql)
        self.conn.commit()
        return item
    def close_spider(self,spider):
        # 关闭数据库的连接
        self.cursor.close()
        self.conn.close()
####################settings.py######################
DB_HOST='192.168.231.140'	# 链接的主机
DB_USER='root'				# 用户
DB_PASSWORD='1234'			# 密码
DB_DATABASE='spider1905'	# 数据库名
DB_PORT=3306				# 端口号
DB_CHARSET='utf8'			# 编码
```

## 9. 日志信息和日志等级

```
1.日志级别
	1.CRITICAL：严重错误
    2.ERROR：一般错误
    3.WARNING：警告
    4.INFO: 一般信息
    5.DEBUG：调试信息
    提示：默认的日志等级是DEBUG,只要出现了DEBUG或者DEBUG以上等级的日志,那么这些日志将会打印
2.settings.py文件设置
	LOG_FILE:将屏幕显示的信息全部记录到文件中，屏幕不再显示，注意文件后缀一定是.log
	LOG_LEVEL:设置日志显示的等级，就是显示哪些，不显示哪些
```

## 10. scrapy的get和post

### 1. scrapy的get

```python
1.scrapy.Request(url=url, callback=self.parse_item, meta={'item': item}, headers=headers)
	url: 要请求的地址
    callback：响应成功之后的回调函数
    meta: 参数传递  接收的语法：item = response.meta['item']
    headers: 定制头信息，一般不用      
    parse_item方法中的response参数就是url执行之后的请求结果
2.response 是一个对象 函数的第二个参数
    response.text: 字符串格式的文本
    response.body: 二进制格式的文本
    response.url: 当前响应的url地址
    response.status: 状态码
    response.xpath(): 筛选你想要的内容
    response.css(): 筛选你想要的内容
```

### 2. scrapy的post请求

```python
1.重写start_requests方法：
		def start_requests(self)
2.start_requests的返回值：
 	 scrapy.FormRequest(url=url, headers=headers, callback=self.parse_item,formdata=data)
         url: 要发送的post地址
         headers：可以定制头信息
         callback: 回调函数   
         formdata: post所携带的数据，这是一个字典
```

### 3. 先post请求再跳转到get请求

```python
#######################自定义的爬虫文件.py######################
# -*- coding: utf-8 -*-
import scrapy

class WbSpider(scrapy.Spider):
    name = 'wb'
    allowed_domains = ['weibo.cn']
	# 登录页面
    def start_requests(self):
        # 路由
        url = 'https://passport.weibo.cn/sso/login'
        # 请求头        
        headers={
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.9',
            'Connection': 'keep-alive',
            'Content-Type': 'application/x-www-form-urlencoded',
            'Cookie': 'SCF=Ahi2Sm3XHpcYIJvIsbJd8AnqkyO8t5RFmHXn8yHeTOMYgumvEqFGsgNbZbD6BmzlV7GA-B8sNWcbTcHeVmF3eNc.; _T_WM=1f1272ae0786a2a2ef34c961901b618b; SUHB=0rrLDdNlZe5bVf; login=d434df472bb5ab59af1d4577b2b5d916',
            'Host': 'passport.weibo.cn',
            'Origin': 'https://passport.weibo.cn',
            'Referer': 'https://passport.weibo.cn/signin/login?entry=mweibo&r=https%3A%2F%2Fweibo.cn%2F&backTitle=%CE%A2%B2%A9&vt=',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36',
        }
        # 参数        
        data={
            'username': '18642820892',
            'password': 'lijing1150501',
            'savestate': '1',
            'r': 'https://weibo.cn/',
            'ec': '0',
            'pagerefer': 'https://weibo.cn/pub/?vt=',
            'entry': 'mweibo',
            'wentry': '',
            'loginfrom': '',
            'client_id': '',
            'code': '',
            'qq': '',
            'mainpageflag': '1',
            'hff': '',
            'hfp': '',
        }
        # 发出登录post请求        
        yield scrapy.FormRequest(url=url,formdata=data,headers=headers,callback=self.parse_second)
	# 登录成功之后
    def parse_second(self,response):
        # url        
        url = 'https://weibo.cn/6451491586/info'
		# 发出修改get请求
        yield scrapy.Request(url=url,callback=self.parse_third)
	# 把获取的数据保存起来
    def parse_third(self,response):
        content = response.text
        with open('wb.html','w',encoding='utf-8')as fp:
            fp.write(content)
##########################settings.py############################
# -*- coding: utf-8 -*-
BOT_NAME = 'weibo'

SPIDER_MODULES = ['weibo.spiders']
NEWSPIDER_MODULE = 'weibo.spiders'

# Crawl responsibly by identifying yourself (and your website) on the user-agent
USER_AGENT = 'User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36'

# Obey robots.txt rules
ROBOTSTXT_OBEY = False
```

## 11. 代理（通过下载中间件来进行添加）

```python
1.到settings.py中，打开一个选项
	DOWNLOADER_MIDDLEWARES = {'postproject.middlewares.Proxy': 543,}
2.到middlewares.py中写代码
	def process_request(self, request, spider):
		request.meta['proxy'] = 'https://113.68.202.10:9999'
		return None
```

## 12. scrapy-redis

### 1. redis安装—— Redis-x64-3.2.100 

```python
1.把压缩包解压到E盘
2.启动
	1.E:
	2.cd Redis\Redis-x64-3.2.100
	3.redis-server redis.windows.conf
	4.设置服务命令redis-server --service-install redis.windows-service.conf --loglevel verbose
3.连接
	1.本地连接 redis-cli
	2.远程连接 第56行直接注释掉 第75行改为protected-mode no
	3. redis-cli -h host -p port      
	   redis-cli -h 10.11.63.79
4.客户端图形化安装
	图形化界面操作redis数据库
	select 0-15 用来切换数据库	
扩展：
	卸载服务：redis-server --service-uninstall
	开启服务：redis-server --service-start
	停止服务：redis-server --service-stop
	官网下载地址：http://redis.io/download
	github下载地址：https://github.com/MSOpenTech/redis/tags
```

### 2. scrapy和scrapy_redis区别？

```
1.scrapy是一个通用的爬虫框架，但是这个框架不支持分布式
2.scrapy_redis就是为了实现scrapy的分布式而诞生的，它提供了一些基于redis的组件
	网址：https://www.cnblogs.com/nick477931661/p/9135497.html
```

### 3. scrapy_redis官方

（https://github.com/rmax/scrapy-redis）

```python
1.继承了RedisSpider
2.from scrapy_redis.spiders import RedisSpider
3.添加一个redis_key = 'myspider:start_urls'
4.注意官网提供的init方法不好用，所以还需要我们自己手动添加allowed_domains
5.在settings中添加
	# 指纹去重
	DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
	# 调度器组件
	SCHEDULER = "scrapy_redis.scheduler.Scheduler"
	# 在爬取的过程中允许暂停
	SCHEDULER_PERSIST = True
	# 指定连接的数据库 如果本地连接就不需要写了
	REDIS_HOST = '10.11.52.62' master端的主机地址
	REDIS_PORT = 6379
6.在settings的管道中添加
	'scrapy_redis.pipelines.RedisPipeline': 400,会将数据添加到redis数据库中
7.DOWNLOAD_DELAY = 1 # 在爬取网站的时候，将这个选项打开，给对方一条活路
8.slave端执行 scrapy runspider mycrawler_redis.py
9.master端向队列中添加起始url
	这个key就是你代码中写的  redis_key
	lpush fen:start_urls 'http://www.dytt8.net/html/gndy/dyzz/index.html'
```

