#  çˆ¬è™«æ€»ç»“

## 1. ç†è®ºçŸ¥è¯†ç‚¹

### 1. åçˆ¬æ‰‹æ®µ

```
1.User-Agent(ç”¨æˆ·ä»£ç†-UA)
2.ä»£ç†IP
	è¥¿æ¬¡ä»£ç†â€”â€”å…è´¹çš„
	å¿«ä»£ç†â€”â€”è¦é’±çš„
	ä»€ä¹ˆæ˜¯é«˜åŒ¿åã€åŒ¿åå’Œé€æ˜ä»£ç†ï¼Ÿå®ƒä»¬æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
	1.ä½¿ç”¨é€æ˜ä»£ç†ï¼Œå¯¹æ–¹æœåŠ¡å™¨å¯ä»¥çŸ¥é“ä½ ä½¿ç”¨äº†ä»£ç†ï¼Œå¹¶ä¸”ä¹ŸçŸ¥é“ä½ çš„çœŸå®IPã€‚
	2.ä½¿ç”¨åŒ¿åä»£ç†ï¼Œå¯¹æ–¹æœåŠ¡å™¨å¯ä»¥çŸ¥é“ä½ ä½¿ç”¨äº†ä»£ç†ï¼Œä½†ä¸çŸ¥é“ä½ çš„çœŸå®IPã€‚
	3.ä½¿ç”¨é«˜åŒ¿åä»£ç†ï¼Œå¯¹æ–¹æœåŠ¡å™¨ä¸çŸ¥é“ä½ ä½¿ç”¨äº†ä»£ç†ï¼Œæ›´ä¸çŸ¥é“ä½ çš„çœŸå®IPã€‚
3.éªŒè¯ç è®¿é—®
	æ‰“ç å¹³å°
		äº‘æ‰“ç å¹³å°
		è¶…çº§é¹°(ğŸ¦…)
4.åŠ¨æ€åŠ è½½ç½‘é¡µ  ç½‘ç«™è¿”å›çš„æ˜¯jsæ•°æ® å¹¶ä¸æ˜¯ç½‘é¡µçš„çœŸå®æ•°æ® 
	seleniumé©±åŠ¨çœŸå®çš„æµè§ˆå™¨å‘é€è¯·æ±‚
5.æ•°æ®åŠ å¯†  
	åˆ†æjsä»£ç 
```

### 2. HTTPåè®®

```
1.HTTPï¼šè¶…æ–‡æœ¬ä¼ è¾“åè®®,ç«¯å£å·80
2.HTTPSï¼šåŠ å¯†ä¼ è¾“,ç«¯å£å·443,HTTPS = HTTP + SSL
3.SSLï¼šå®‰å…¨ å¥—æ¥å±‚
	å¦‚æœæŠ¥é”™SSL,è§£å†³æ–¹æ¡ˆæ˜¯ï¼š
	import urllib.request
		import ssl
		ssl._create_default_https_context = ssl._create_unverified_context	
4.å¸¸è§æœåŠ¡å™¨ç«¯å£å·
	FTP			21
	SSH			22
	MySQL		3306
	Oracle		1521
	MongoDB 	27017
	Redis		6379
5.urlç»„æˆ
	hhtp://	www.baidu.com:80/	findUser/ ?name=zs&age=18/	#
	åè®®	  ä¸»æœºIPåœ°å€	ç«¯å£å·	   èµ„æºè·¯å¾„		å‚æ•°			é”šç‚¹	
```

## 2. urllibåº“

### 1. urllibåŸºæœ¬ä½¿ç”¨

```python
import urllib.request

url = 'http://www.baidu.com'
# æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
response = urllib.request.urlopen(url=url)
print(response)
# <http.client.HTTPResponse object at 0x000002A3DBF0F240>
print(type(response))
# <class 'http.client.HTTPResponse'>
```

### 2. æœåŠ¡å™¨å“åº”

```python
import urllib.request

# readæ–¹æ³•	readlineæ–¹æ³•	readlinesæ–¹æ³•
# getcodeæ–¹æ³•	geturlæ–¹æ³•	getheadersæ–¹æ³•

url = 'http://www.baidu.com'
# æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
response = urllib.request.urlopen(url=url)

# 1.readæ–¹æ³•
# ä»¥äºŒè¿›åˆ¶çš„æ ¼å¼æ¥è·å–ç½‘ç«™æºç 
print(response.read())
# è¯»å–å‰5ä¸ªå­—èŠ‚
print(response.read(5))
# ä»¥å­—ç¬¦ä¸²çš„æ ¼å¼æ¥è¯»å–ç½‘ç«™æºç 
print(response.read().decode('utf-8'))

# 2.readlineæ–¹æ³•
# ä»¥äºŒè¿›åˆ¶æ ¼å¼ä¸€è¡Œä¸€è¡Œçš„è¯»å–
a = response.readline()
print(a)
# è¯»å–è¿™ä¸€è¡Œçš„å‰5ä¸ªå­—èŠ‚
a = response.readline(5)
print(a)

# 3.readlinesæ–¹æ³•
# ä¸€è¡Œä¸€è¡Œçš„è¯»å–ï¼Œå…¨éƒ¨æ”¾åˆ°åˆ—è¡¨ä¸­
a = response.readlines()
print(a)

# 4.getcodeæ–¹æ³•
#è·å–å“åº”çš„çŠ¶æ€ç 
print(response.getcode())

# 5.geturlæ–¹æ³•
# è·å–url
print(response.geturl())

# 6.getheadersæ–¹æ³•
# è·å–è¯·æ±‚å¤´ä¿¡æ¯
print(response.getheaders())
```

### 3. ä¸‹è½½

```python
import urllib.request

# ä¸‹è½½ç½‘é¡µ
url = 'http://www.baidu.com'
# urlæ˜¯è®¿é—®çš„è·¯å¾„  filenameæ˜¯ä¸‹è½½åˆ°æœ¬åœ°ä¹‹åçš„åå­—
urllib.request.urlretrieve(url=url,filename='bd.html')

# ä¸‹è½½å›¾ç‰‡
url = 'https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1571735604081&di=aa81344782326fc8e9f40e09aba8667f&imgtype=0&src=http%3A%2F%2Fimage.aiwenwo.net%2Fwww%2Ftvgps%2Fuploads%2Fallimg%2F160408%2F24-16040Q60305V8.jpg'
urllib.request.urlretrieve(url=url,filename='ycxWIFE.jpg')

# ä¸‹è½½è§†é¢‘
url = 'http://vd3.bdstatic.com/mda-ih0qi72n1mc0miwh/sc/mda-ih0qi72n1mc0miwh.mp4'
urllib.request.urlretrieve(url=url,filename='xiaoshipin.mp4')
```

### 4. è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶

```python
import urllib.request

# é»˜è®¤æƒ…å†µä¸‹ urllib.request.urlopenæ–¹æ³• è®¿é—®æœåŠ¡å™¨æºå¸¦çš„uaæ˜¯pythonçš„ua
# é‚£ä¹ˆå¾ˆå¤šç½‘ç«™ä¼šæ£€æµ‹uaçš„çœŸå®æ€§  å¦‚æœæ˜¯ä¸€ä¸ªçˆ¬è™«ç¨‹åº é‚£ä¹ˆå°†ä¸ä¼šç»™ä½ è¿”å›æ•°æ®
url = 'http://www.baidu.com'
# è¯·æ±‚å¤´
headers={
'User-Agent':'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50'
}
# urlopenæ–¹æ³•ä¸­æ˜¯æ²¡æœ‰headersçš„å±æ€§çš„ æ‰€ä»¥ä¸å¯ä»¥ç›´æ¥åœ¨è¯¥æ–¹æ³•ä¸­æ·»åŠ ua
# response = urllib.request.urlopen(url=url,headers=headers)
request = urllib.request.Request(url=url,headers=headers)
# æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
response = urllib.request.urlopen(request)
print(response.getcode())
```

### 5. ç¼–è§£ç 

#### 1. get

+ getè¯·æ±‚æ–¹å¼å¾—ç¼–ç (ä¸€ä¸ªå‚æ•°å¸¦ä¸­æ–‡name)

  ```python
  import urllib.request
  import urllib.parse
  
  # é»˜è®¤æƒ…å†µä¸‹  æµè§ˆå™¨ä¼šè‡ªåŠ¨å¾—è¿›è¡Œç¼–è§£ç   å½“è®²æ±‰è¯­å¤åˆ¶åˆ°pycharmä¸­å¾—
  # æ—¶å€™ï¼Œé‚£ä¹ˆpycharmä¸ä¼šè‡ªåŠ¨ç¼–è§£ç 
  url = 'https://www.baidu.com/s?wd='
  name = 'éŸ©çº¢'
  # å°†nameè¿›è¡Œç¼–ç 
  name = urllib.request.quote(name)
  # urlå’Œnameè¿›è¡Œæ‹¼æ¥
  url = url + name
  # æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
  response = urllib.request.urlopen(url=url)
  # è¯»å–æœåŠ¡å™¨å“åº”çš„æ•°æ®
  t = response.read().decode('utf-8')
  print(t)
  ```

+ getè¯·æ±‚å¯¹è±¡çš„å®šåˆ¶

  ```python
  import urllib.request
  import urllib.parse
  
  # url   dataå‚æ•°  headers
  url='http://www.baidu.com/s?wd='
  # ç¼–ç 
  data = '1906ç­'
  data = urllib.parse.quote(data)
  # æ‹¼æ¥
  url = url + data
  
  # è¯·æ±‚å¤´ua
  headers={
  'User-Agent': 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50'
  }
  
  # è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
  request = urllib.request.Request(url=url,headers=headers)
  # æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
  response = urllib.request.urlopen(request)
  # è¯»å–æœåŠ¡å™¨å“åº”çš„æ•°æ®
  p_source = response.read().decode('utf-8')
  print(p_source)
  ```

+ getè¯·æ±‚å¤šä¸ªå‚æ•°ç¼–è§£ç 

  ```python
  # å¤šä¸ªå‚æ•°æ‰§è¡Œgetè¯·æ±‚
  # http://www.baidu.com/s?name=éŸ©ç¾å¨Ÿ&sex=ä¸çŸ¥é“
  
  import urllib.request
  import urllib.parse
  
  url = 'http://www.baidu.com/s?'
  # ç¼–ç 
  data = {
      'wd':'éŸ©ç¾å¨Ÿ',
      'sex':'ä¸çŸ¥é“',
  }
  data = urllib.parse.urlencode(data)
  # æ‹¼æ¥
  url = url + data
  # è¯·æ±‚å¤´
  headers = {
      'User-Agent': 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50'
  }
  # è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
  request = urllib.request.Request(url=url,headers=headers)
  # æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
  response = urllib.request.urlopen(request)
  # è¯»å–æœåŠ¡å™¨å“åº”çš„æ•°æ®
  print(response.read().decode('utf-8'))
  ```

#### 2. post

+ postè¯·æ±‚â€”â€”ä¸€ä¸ªå‚æ•°

  ```python
  import urllib.request
  import urllib.parse
  
  url = 'https://fanyi.baidu.com/sug'
  # ç¼–ç 
  data = {
  	'kw':'shite'
  }
  # postè¯·æ±‚å‚æ•°å¿…é¡»æœ‰encode('utf-8')ç¼–ç 
  data = urllib.parse.urlencode(data).encode('utf-8')
  # è¯·æ±‚å¤´
  headers = {
  'User-Agent': 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50'
  }
  # è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
  request = urllib.request.Request(url=url,headers=headers,data=data)
  # æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
  response = urllib.request.urlopen(request)
  # è¯»å–æœåŠ¡å™¨å“åº”çš„æ•°æ®
  content = response.read().decode('utf-8')
  
  import json
  # æŠŠäºŒè¿›åˆ¶è½¬æ¢æˆä¸­æ–‡
  obj = json.loads(content)
  s = json.dumps(obj,ensure_ascii=False)
  print(s)
  ```

+ postè¯·æ±‚â€”â€”ç™¾åº¦ç¿»è¯‘è¯¦ç»†ç¿»è¯‘

  ```python
  import urllib.request
  import urllib.parse
  
  url = 'https://fanyi.baidu.com/v2transapi?from=en&to=zh'
  # ç¼–ç 
  data ={
      'from': 'en',
      'to': 'zh',
      'query': 'dream',
      'transtype': 'realtime',
      'simple_means_flag': '3',
      'sign': '679690.965691',
      'token': '1368ee2ecb7fd8616a2c928e8b0b3abb',
  }
  data = urllib.parse.urlencode(data).encode('utf-8')
  # è¯·æ±‚å¤´
  headers = {
      'User-Agent': 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50'
  }
  # è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
  request = urllib.request.Request(url=url,headers=headers,data=data)
  # æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
  response = urllib.request.urlopen(request)
  # è¯»å–æœåŠ¡å™¨å“åº”çš„æ•°æ®
  content = response.read().decode('utf-8')
  
  import json
  # æŠŠäºŒè¿›åˆ¶è½¬æ¢æˆä¸­æ–‡
  obj = json.loads(content)
  s = json.dumps(obj,ensure_ascii=False)
  print(s)
  ```

#### 3. ajax

+ postå’ŒgetåŒºåˆ«

  ```python 
  1ï¼šgetè¯·æ±‚æ–¹å¼çš„å‚æ•°å¿…é¡»ç¼–ç ï¼Œå‚æ•°æ˜¯æ‹¼æ¥åˆ°urlåé¢ï¼Œç¼–ç ä¹‹åä¸éœ€è¦è°ƒç”¨encodeæ–¹æ³•
  2ï¼špostè¯·æ±‚æ–¹å¼çš„å‚æ•°å¿…é¡»ç¼–ç ï¼Œå‚æ•°æ˜¯æ”¾åœ¨è¯·æ±‚å¯¹è±¡å®šåˆ¶çš„æ–¹æ³•ä¸­ï¼Œç¼–ç ä¹‹åéœ€è¦è°ƒç”¨encodeæ–¹æ³•
  ```

+ ajaxçš„getè¯·æ±‚â€”â€”è±†ç“£ç”µå½±å•é¡µä¸‹è½½

  ```python
  import urllib.request
  
  url = 'https://movie.douban.com/j/chart/top_list?type=6&interval_id=100%3A90&action=&start=0&limit=20'
  # è¯·æ±‚å¤´
  headers = {
  'User-Agent': 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50'
  }
  # è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
  request = urllib.request.Request(url=url,headers=headers)
  # æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
  response = urllib.request.urlopen(request)
  # è¯»å–æœåŠ¡å™¨å“åº”çš„æ•°æ®
  content = response.read().decode('utf-8')
  # è¯»å–çš„æ•°æ®å†™å…¥åˆ°æ–‡ä»¶ä¸­
  with open('douban.json','w',encoding='utf-8')as fp:
  	fp.write(content)
  ```

+ ajaxçš„getè¯·æ±‚â€”â€”è±†ç“£ç”µå½±å¤šé¡µä¸‹è½½

  ```python
  import urllib.request
  import urllib.parse
  
  # è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
  def create_request(page):
      url = 'https://movie.douban.com/j/chart/top_list?'
      # ç¼–ç 
      data = {
          'type': '25',
          'interval_id': '100:90',
          'action':'',
          # 1 0  2 20  3 40
          'start': (page-1)*20,
          'limit': '20'
      }
      data = urllib.parse.urlencode(data)
      # æ‹¼æ¥
      url = url + data
      # è¯·æ±‚å¤´
      headers = {
          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
      }
      # è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
      request = urllib.request.Request(url=url,headers=headers)
      return request
  
  # è·å–é¡µé¢çš„æºç 
  def get_content(request):
      # æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
      response = urllib.request.urlopen(request)
      # è¯»å–æœåŠ¡å™¨å“åº”çš„æ•°æ®
      content = response.read().decode('utf-8')
      return content
  
  # è¯»å–çš„æ•°æ®å†™å…¥åˆ°æ–‡ä»¶ä¸­
  def down_load(page,content):
      with open('douban_'+str(page)+'.json','w',encoding='utf-8')as fp:
          fp.write(content)
  
  if __name__ == '__main__':
      start_page = int(input('è¯·è¾“å…¥èµ·å§‹é¡µç '))
      end_page = int(input('è¯·è¾“å…¥ç»“æŸé¡µç '))
  
      for page in range(start_page,end_page+1):
          request = create_request(page)
          content = get_content(request)
          down_load(page,content)
  ```

+ ajaxçš„postè¯·æ±‚â€”â€”KFCå®˜ç½‘

  ```python
  import urllib.parse
  import urllib.request
  
  # è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶ 
  def create_request(page):
      url = 'http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname'
      # ç¼–ç 
      data = {
          'cname': 'å¤§è¿',
          'pid':'',
          'pageIndex': page,
          'pageSize': '10',
      }
      # postè¯·æ±‚å‚æ•°å¿…é¡»æœ‰encode('utf-8')ç¼–ç 
      data = urllib.parse.urlencode(data).encode('utf-8')
      # è¯·æ±‚å¤´
      headers = {
          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
      }
      # è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
      request = urllib.request.Request(url=url,headers=headers,data=data)
      return request
  
  # è·å–é¡µé¢çš„æºç 
  def get_content(request):
      # æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
      response = urllib.request.urlopen(request)
      # è¯»å–æœåŠ¡å™¨å“åº”çš„æ•°æ®
      content = response.read().decode('utf-8')
      return content
  
  # è¯»å–çš„æ•°æ®å†™å…¥åˆ°æ–‡ä»¶ä¸­
  def down_load(page,content):
      with open('kfc_'+str(page)+'.json','w',encoding='utf-8')as fp:
          fp.write(content)
  
  if __name__ == '__main__':
      start_page = int(input('è¯·è¾“å…¥èµ·å§‹é¡µç '))
      end_page = int(input('è¯·è¾“å…¥ç»“æŸé¡µç '))
      for page in range(start_page,end_page+1):
          request = create_request(page)
          content = get_content(request)
          down_load(page,content)
  ```

### 6. å¼‚å¸¸  HTTPError  URLError

```python
import urllib.request
import urllib.error
# åŸŸåæˆ–è€…IPåœ°å€é”™è¯¯æŠ¥URLError
# è·¯å¾„é”™è¯¯æŠ¥HTTPError
url = 'https://blog.csdn.net/ityard/article/details/102646738'
# è¯·æ±‚å¤´
headers = {    
        'Cookie': 'uuid_tt_dd=10_19284691370-1530006813444-566189; smidV2=2018091619443662be2b30145de89bbb07f3f93a3167b80002b53e7acc61420; _ga=GA1.2.1823123463.1543288103; dc_session_id=10_1550457613466.265727; acw_tc=2760821d15710446036596250e10a1a7c89c3593e79928b22b3e3e2bc98b89; Hm_lvt_e5ef47b9f471504959267fd614d579cd=1571329184; Hm_ct_e5ef47b9f471504959267fd614d579cd=6525*1*10_19284691370-1530006813444-566189; __yadk_uid=r0LSXrcNYgymXooFiLaCGt1ahSCSxMCb; Hm_lvt_6bcd52f51e9b3dce32bec4a3997715ac=1571329199,1571329223,1571713144,1571799968; acw_sc__v2=5dafc3b3bc5fad549cbdea513e330fbbbee00e25; firstDie=1; SESSION=396bc85c-556b-42bd-890c-c20adaaa1e47; UserName=weixin_42565646; UserInfo=d34ab5352bfa4f21b1eb68cdacd74768; UserToken=d34ab5352bfa4f21b1eb68cdacd74768; UserNick=weixin_42565646; AU=7A5; UN=weixin_42565646; BT=1571800370777; p_uid=U000000; dc_tos=pzt4xf; Hm_lpvt_6bcd52f51e9b3dce32bec4a3997715ac=1571800372; Hm_ct_6bcd52f51e9b3dce32bec4a3997715ac=1788*1*PC_VC!6525*1*10_19284691370-1530006813444-566189!5744*1*weixin_42565646; announcement=%257B%2522isLogin%2522%253Atrue%252C%2522announcementUrl%2522%253A%2522https%253A%252F%252Fblogdev.blog.csdn.net%252Farticle%252Fdetails%252F102605809%2522%252C%2522announcementCount%2522%253A0%252C%2522announcementExpire%2522%253A3600000%257D',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36',
}

try:
    # è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
    request = urllib.request.Request(url=url,headers=headers)
	# æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
    response = urllib.request.urlopen(request)
	# è¯»å–æœåŠ¡å™¨å“åº”çš„æ•°æ®
    content = response.read().decode('utf-8')
    print(content)
except urllib.error.HTTPError:
    print(1111)
except urllib.error.URLError:
    print(2222)
```

### 7. cookieç™»å½•

+ ç™»å½•äººäººç½‘(get)

  ```python
  import urllib.request
  import urllib.parse
  
  url = 'http://www.renren.com/305523888/profile'
  # è¯·æ±‚å¤´
  headers = {
          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36',
          'Cookie': 'anonymid=jix3nuu4-498h3n; _de=BF83005E46A2ACDF72FFEFECAA50653A696BF75400CE19CC; ln_uact=595165358@qq.com; ln_hurl=http://hdn.xnimg.cn/photos/hdn521/20170509/0940/main_5crY_aee9000088781986.jpg; _r01_=1; jebe_key=b8a3f973-563c-4e6a-ac8f-99deef080f20%7Ccfcd208495d565ef66e7dff9f98764da%7C1565664618046%7C0%7C1565664616181; wp_fold=0; depovince=SH; jebecookies=dd7a7840-774a-44e8-89ab-36942629e3af|||||; JSESSIONID=abc61vSCzpF2xhumYp33w; ick_login=f9f873c2-206c-4c17-9cb9-ec4c7e3044d9; p=b178623a64102cc30833baf87c1c33f28; first_login_flag=1; t=ec055348320811e3b0b3345d15afe18c8; societyguester=ec055348320811e3b0b3345d15afe18c8; id=305523888; xnsid=8bff41c; loginfrom=syshome; jebe_key=b8a3f973-563c-4e6a-ac8f-99deef080f20%7Cdca572dcc866b00768c874af75fd79ec%7C1571811553654%7C1%7C1571811557213'
  }
  # è¯·æ±‚å¯¹è±¡çš„å¯¹è±¡
  request = urllib.request.Request(url=url,headers=headers)
  # æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
  response = urllib.request.urlopen(request)
  # è¯»å–æœåŠ¡å™¨å“åº”çš„æ•°æ®
  content = response.read().decode('utf-8')
  print(content)
  ```

+ ç™»å½•å¾®åš(get)

  ```python
  import urllib.request
  
  url = 'https://weibo.cn/6451491586/info'
  # è¯·æ±‚å¤´
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36',
  	'cookie': 'SCF=Ahi2Sm3XHpcYIJvIsbJd8AnqkyO8t5RFmHXn8yHeTOMYgumvEqFGsgNbZbD6BmzlV7GA-B8sNWcbTcHeVmF3eNc.; _T_WM=661e0af7353e4ce48f5c9cfa8368bce5; SUB=_2A25wq4T0DeRhGeBK7lMV-S_JwzqIHXVQVyy8rDV6PUJbkdAKLXTakW1NR6e0UGDjQ3X7s7TSUa_J_IHy8f7dJeXJ; SUHB=0idbj3TaBzD_2G; SSOLoginState=1571812516',
  }
  # è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
  request = urllib.request.Request(url=url,headers=headers)
  # æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
  response = urllib.request.urlopen(request)
  # è¯»å–æœåŠ¡å™¨å“åº”çš„æ•°æ®
  content = response.read().decode('utf-8')
  # è¯»å–çš„æ•°æ®å†™å…¥æ–‡ä»¶ä¸­
  with open('weibo.html','w',encoding='utf-8')as fp:
      fp.write(content)
  ```

+ ç™»å½•å¾®åšâ€”â€”ç”¨æˆ·åç¼“å­˜(post)

  ```python
  import urllib.request
  import urllib.parse
  
  url = 'https://passport.weibo.cn/sso/login'
  # ç¼–ç 
  data = {
      'username': '18642820892',
      'password': 'lijing1150501',
      'savestate': '1',
      'r': 'https://weibo.cn/',
      'ec': '0',
      'pagerefer': 'https://weibo.cn/pub/?vt=',
      'entry': 'mweibo',
      'wentry': '',
      'loginfrom': '',
      'client_id': '',
      'code': '',
      'qq': '',
      'mainpageflag': '1',
      'hff': '',
      'hfp':'',
  }
  data = urllib.parse.urlencode(data).encode('utf-8')
  # è¯·æ±‚å¤´
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36',
      # æ£€æµ‹ä¸Šä¸€çº§é¡µé¢
  	'Referer': 'https://passport.weibo.cn/signin/login?entry=mweibo&r=https%3A%2F%2Fweibo.cn%2F&backTitle=%CE%A2%B2%A9&vt='
  }
  # è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
  request = urllib.request.Request(url=url,headers=headers,data=data)
  # æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
  response = urllib.request.urlopen(request)
  # è¯»å–æœåŠ¡å™¨å“åº”çš„æ•°æ®
  content = response.read().decode('utf-8')
  print(content)
  ```


### 8. handlerâ€”â€”å®šåˆ¶æ›´é«˜çº§çš„è¯·æ±‚å¤´

```python
import urllib.request

url = 'http://www.baidu.com'
# è¯·æ±‚å¤´
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
}
# è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
request = urllib.request.Request(url=url,headers=headers)

# ä¸‹é¢çš„ç›¸å½“äºurllib.request.urlopen(request)
handler = urllib.request.HTTPHandler()
opener = urllib.request.build_opener(handler)
response = opener.open(request)
# è¯»å–æœåŠ¡å™¨å“åº”çš„æ•°æ®
content = response.read().decode('utf-8')
print(content)
```

### 9. IPä»£ç†

+ è¥¿æ¬¡ä»£ç†â€”â€”å…è´¹çš„

  ```python
  import urllib.request
  
  url = 'https://www.baidu.com/s?wd=ip'
  # è¯·æ±‚å¤´
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
  }
  # è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
  request = urllib.request.Request(url=url,headers=headers)
  
  # IPä»£ç†
  proxies = {
      'https':'210.26.49.88:3128'
  }
  # ä¸‹é¢çš„ç›¸å½“äºresponse = urllib.request.urlopen(request)
  handler = urllib.request.ProxyHandler(proxies=proxies)
  opener = urllib.request.build_opener(handler)
  response = opener.open(request)
  # è¯»å–æœåŠ¡å™¨å“åº”çš„æ•°æ®
  content = response.read().decode('utf-8')
  # è¯»å–çš„æ•°æ®å†™å…¥æ–‡ä»¶ä¸­
  with open('daili.html','w',encoding='utf-8')as fp:
      fp.write(content)
  ```

+ å¿«ä»£ç†â€”â€”æ”¶è´¹çš„

  ```python
  import urllib.request
  
  # è·å–è‡ªå·±è´­ä¹°çš„ä»£ç†IP
  url_ip='http://kps.kdlapi.com/api/getkps/?orderid=987181651846636&num=1&pt=1&sep=1'
  response = urllib.request.urlopen(url_ip)
  content_ip = response.read().decode('utf-8')
  
  url = 'https://www.baidu.com/s?wd=ip'
  # è¯·æ±‚å¤´
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
  }
  # è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
  request = urllib.request.Request(url=url,headers=headers)
  # IPä»£ç†
  proxies = {
      'https':content_ip
  }
  # ä¸‹é¢çš„ç›¸å½“äºresponse = urllib.request.urlopen(request)
  handler = urllib.request.ProxyHandler(proxies=proxies)
  opener = urllib.request.build_opener(handler)
  response = opener.open(request)
  # è¯»å–æœåŠ¡å™¨å“åº”çš„æ•°æ®
  content = response.read().decode('utf-8')
  # è¯»å–çš„æ•°æ®å†™å…¥æ–‡ä»¶ä¸­
  with open('daili1.html','w',encoding='utf-8')as fp:
      fp.write(content)
  ```

+ ä»£ç†æ± â€”â€”æ ¡èŠ±ç½‘

  ```python
  import urllib.request
  import random
  
  url = 'http://www.xiaohuar.com'
  # è¯·æ±‚å¤´
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
  }
  # è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
  request = urllib.request.Request(url=url,headers=headers)
  # å®šä¹‰äº†ä¸€ä¸ªä»£ç†çš„ä¸€ä¸ªåˆ—è¡¨ æˆ‘ä»¬ç§°ä¹‹ä¸ºä»£ç†æ± 
  proxy = [
      {'http':'112.74.108.33:16818'},
      {'http': '112.74.108.33:16818'},
      {'http': '112.74.108.33:16818'},
  ]
  # éšæœºä¼šæŠ½å–ä¸€ä¸ªä»£ç†
  proxies = random.choice(proxy)
  
  # ä½¿ç”¨hanlerä»£ç†
  handler = urllib.request.ProxyHandler(proxies=proxies)
  opener = urllib.request.build_opener(handler)
  response = opener.open(request)
  # è·å–å“åº”æ•°æ®
  content = response.read().decode('utf-8')
  # ä¿å­˜é¡µé¢
  with open('xh.html','w',encoding='utf-8')as fp:
      fp.write(content)
  ```

### 10. cookieåŠ¨æ€è·å–

```python
import urllib.request
import urllib.parse
import http.cookiejar
# cookiejar  æ¯ä¸€æ¬¡ç™»é™†ä¹‹åçš„cookieéƒ½æ˜¯ä¸ä¸€æ ·çš„  é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•è§£å†³ç™»é™†é—®é¢˜

url_post = 'http://www.quanshuwang.com/login.php?do=submit'
# ç¼–ç 
data = {
    'username': 'action',
    'password': 'action',
    'action': 'login',
}
data = urllib.parse.urlencode(data).encode('gbk')
# è¯·æ±‚å¤´
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
}
# è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
request_post = urllib.request.Request(url=url_post,data=data,headers=headers)

# å½“ä½¿ç”¨äº†opener.openå‘é€äº†è¯·æ±‚  é‚£ä¹ˆä¼šå°†ç™»é™†çš„cookieä¿å­˜åˆ°openeræ‰€åœ¨cookiejarä¸­
cookiejar = http.cookiejar.CookieJar()
handler = urllib.request.HTTPCookieProcessor(cookiejar=cookiejar)
opener = urllib.request.build_opener(handler)
response = opener.open(request_post)

url_get = 'http://www.quanshuwang.com/modules/article/bookcase.php'
request_get = urllib.request.Request(url=url_get,headers=headers)
response = opener.open(request_get)
content = response.read().decode('gbk')
print(content)
```

## 3. æ•°æ®è§£æ

### 1. æ­£åˆ™è¡¨è¾¾å¼

+ å•å­—ä¿®é¥°ç¬¦

  ```python
  1. åŒ¹é…ä»»æ„å­—ç¬¦ï¼Œé™¤äº†æ¢è¡Œç¬¦
  2. []  ç”¨æ¥è¡¨ç¤ºä¸€ç»„å­—ç¬¦,å•ç‹¬åˆ—å‡ºï¼š[abc] åŒ¹é… 'a'ï¼Œ'b'æˆ–'c'
  3. \d  åŒ¹é…ä»»æ„æ•°å­—ï¼Œç­‰ä»·äº [0-9].
  4. \D  åŒ¹é…ä»»æ„éæ•°å­—
  5. \w  åŒ¹é…å­—æ¯æ•°å­—åŠä¸‹åˆ’çº¿
  6. \W  åŒ¹é…éå­—æ¯æ•°å­—åŠä¸‹åˆ’çº¿
  7. \s  åŒ¹é…ä»»æ„ç©ºç™½å­—ç¬¦ï¼Œç­‰ä»·äº [\t\n\r\f].
  8. \S  åŒ¹é…ä»»æ„éç©ºå­—ç¬¦
  ```

+ æ•°é‡ä¿®é¥°ç¬¦

  ```python
  1. *    åŒ¹é…0ä¸ªæˆ–å¤šä¸ªçš„è¡¨è¾¾å¼	
  2. +    åŒ¹é…1ä¸ªæˆ–å¤šä¸ªçš„è¡¨è¾¾å¼
  3. ?    åŒ¹é…0ä¸ªæˆ–1ä¸ªç”±å‰é¢çš„æ­£åˆ™è¡¨è¾¾å¼å®šä¹‰çš„ç‰‡æ®µ
  4. {m}  å‰é¢å­—ç¬¦å‡ºç°mæ¬¡
  5. {m,} å‰é¢å­—ç¬¦å‡ºç°è‡³å°‘mæ¬¡
  6. {m,n}å‰é¢å­—ç¬¦å‡ºç°m~næ¬¡
  ```

+ è¾¹ç•Œä¿®é¥°ç¬¦

  ```python
  1. ^    ä»¥...å¼€å§‹		eg:'^abc' åŒ¹é…ä»¥abcå¼€å¤´
  2. $    ä»¥...ç»“å°¾		eg:'abc$'  åŒ¹é…ä»¥abcç»“å°¾
  ```

+ åˆ†ç»„ä¿®é¥°ç¬¦

  ```python
  1.() åŒ¹é…æ‹¬å·å†…çš„è¡¨è¾¾å¼ï¼Œä¹Ÿè¡¨ç¤ºä¸€ä¸ªç»„	
  2.\1  \2  åŒ¹é…ç¬¬1ã€2ä¸ªåˆ†ç»„çš„å†…å®¹
  eg:ï¼ˆ.*ï¼‰:(.*)	
  	 \1    \2
  ```

+ è´ªå©ªæ¨¡å¼/éè´ªå©ªæ¨¡å¼

  ```python
  è´ªå©ªæ¨¡å¼ï¼šåœ¨æ•´ä¸ªè¡¨è¾¾å¼åŒ¹é…æˆåŠŸçš„å‰æä¸‹ï¼Œå°½å¯èƒ½å¤šçš„åŒ¹é… ( * )ï¼›
  éè´ªå©ªæ¨¡å¼ï¼šåœ¨æ•´ä¸ªè¡¨è¾¾å¼åŒ¹é…æˆåŠŸçš„å‰æä¸‹ï¼Œå°½å¯èƒ½å°‘çš„åŒ¹é… ( ? )ï¼›
  Pythoné‡Œæ•°é‡è¯é»˜è®¤æ˜¯è´ªå©ªçš„ã€‚
  ç¤ºä¾‹ä¸€ ï¼š æºå­—ç¬¦ä¸²ï¼šabbbc
  ä½¿ç”¨è´ªå©ªçš„æ•°é‡è¯çš„æ­£åˆ™è¡¨è¾¾å¼ ab* ï¼ŒåŒ¹é…ç»“æœï¼š abbbã€‚ 
  * å†³å®šäº†å°½å¯èƒ½å¤šåŒ¹é… bï¼Œæ‰€ä»¥aåé¢æ‰€æœ‰çš„ b éƒ½å‡ºç°äº†ã€‚
  ä½¿ç”¨éè´ªå©ªçš„æ•°é‡è¯çš„æ­£åˆ™è¡¨è¾¾å¼ab*?ï¼ŒåŒ¹é…ç»“æœï¼š aã€‚ 
  å³ä½¿å‰é¢æœ‰ *ï¼Œä½†æ˜¯ ? å†³å®šäº†å°½å¯èƒ½å°‘åŒ¹é… bï¼Œæ‰€ä»¥æ²¡æœ‰ bã€‚
  
  ç¤ºä¾‹äºŒ ï¼š æºå­—ç¬¦ä¸²ï¼šaa<div>test1</div>bb<div>test2</div>cc
  ä½¿ç”¨è´ªå©ªçš„æ•°é‡è¯çš„æ­£åˆ™è¡¨è¾¾å¼ï¼š<div>.*</div>
  åŒ¹é…ç»“æœï¼š<div>test1</div>bb<div>test2</div>
  è¿™é‡Œé‡‡ç”¨çš„æ˜¯è´ªå©ªæ¨¡å¼ã€‚åœ¨åŒ¹é…åˆ°ç¬¬ä¸€ä¸ªâ€œ</div>â€æ—¶å·²ç»å¯ä»¥ä½¿æ•´ä¸ªè¡¨è¾¾
  å¼åŒ¹é…æˆåŠŸï¼Œä½†æ˜¯ç”±äºé‡‡ç”¨çš„æ˜¯è´ªå©ªæ¨¡å¼ï¼Œæ‰€ä»¥ä»ç„¶è¦å‘å³å°è¯•åŒ¹é…ï¼Œ
  æŸ¥çœ‹æ˜¯å¦è¿˜æœ‰æ›´é•¿çš„å¯ä»¥æˆåŠŸåŒ¹é…çš„å­ä¸²ã€‚åŒ¹é…åˆ°ç¬¬äºŒä¸ªâ€œ</div>â€åï¼Œ
  å‘å³å†æ²¡æœ‰å¯ä»¥æˆåŠŸåŒ¹é…çš„å­ä¸²ï¼ŒåŒ¹é…ç»“æŸï¼ŒåŒ¹é…ç»“æœä¸º
  â€œ<div>test1</div>bb<div>test2</div>â€
  
  ä½¿ç”¨éè´ªå©ªçš„æ•°é‡è¯çš„æ­£åˆ™è¡¨è¾¾å¼ï¼š<div>.*?</div>
  åŒ¹é…ç»“æœï¼š<div>test1</div>
  æ­£åˆ™è¡¨è¾¾å¼äºŒé‡‡ç”¨çš„æ˜¯éè´ªå©ªæ¨¡å¼ï¼Œåœ¨åŒ¹é…åˆ°ç¬¬ä¸€ä¸ªâ€œ</div>â€
  æ—¶ä½¿æ•´ä¸ªè¡¨è¾¾å¼åŒ¹é…æˆåŠŸï¼Œç”±äºé‡‡ç”¨çš„æ˜¯éè´ªå©ªæ¨¡å¼ï¼Œ
  æ‰€ä»¥ç»“æŸåŒ¹é…ï¼Œä¸å†å‘å³å°è¯•ï¼ŒåŒ¹é…ç»“æœä¸ºâ€œ<div>test1</div>â€ã€‚
  ```

+ æ¨¡å¼ä¿®é¥°ç¬¦

  ```python
  1. re.S  å•è¡Œæ¨¡å¼
  2. re.M  å¤šè¡Œæ¨¡å¼
  3. re.I å¿½ç•¥å¤§å°å†™
  ```

### 2. æ­£åˆ™è§£æ

+ æ­£åˆ™è¡¨è¾¾å¼ç®€å•ä½¿ç”¨

  ```python
  import re
  
  content = '''
      <div class="thumb">
          <a href="/article/119749308" target="_blank">
          <img src="//pic.qiushibaike.com/system/pictures/11974/119749308/medium/app119749308.jpg" alt="ç³—äº‹#119749308" class="illustration" width="100%" height="auto">
          </a>
      </div>
      <div class="thumb">
          <a href="/article/119750010" target="_blank">
          <img src="//pic.qiushibaike.com/system/pictures/11975/119750010/medium/app119750010.jpg" alt="ç³—äº‹#119750010" class="illustration" width="100%" height="auto">
          </a>
      </div>
      <div class="thumb">
          <a href="/article/121859652" target="_blank">
          <img src="//pic.qiushibaike.com/system/pictures/12185/121859652/medium/LZB48T44DNIAY1CV.jpg" alt="ç³—äº‹#121859652" class="illustration" width="100%" height="auto">
          </a>
      </div>
  '''
  # pattern = re.compile('pic.*?\.jpg')
  # ä½¿ç”¨æ­£åˆ™åŒ¹é…ç»“æœ
  pattern = re.compile('<div class="thumb">.*?<img src="(.*?)" alt=".*?"',re.S)
  src_list = pattern.findall(content)
  for src in src_list:
      print(src)
  ```

+ æ­£åˆ™è¡¨è¾¾å¼çš„ä½¿ç”¨â€”â€”çˆ¬å–ç«™é•¿ç´ æ(å›¾ç‰‡)

  ```python
  import urllib.request
  import re
  
  # è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
  def create_request(page):
      base_url = 'http://sc.chinaz.com/tupian/qinglvtupian'
      if page == 1:
          url = base_url + '.html'
      else:
          url = base_url + '_' + str(page) + '.html'
  	# è¯·æ±‚å¤´
      headers = {
          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
      }
  	# è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
      request = urllib.request.Request(url=url,headers=headers)
      return request
  
  # è·å–é¡µé¢çš„æºç 
  def get_content(request):
      # æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
      response = urllib.request.urlopen(request)
      # è¯»å–æœåŠ¡å™¨å“åº”çš„æ•°æ®
      content = response.read().decode('utf-8')
      # print(content)
      return content
  
  # è¯»å–çš„æ•°æ®å†™å…¥åˆ°æ–‡ä»¶ä¸­
  def down_load(content):
      # pattern = re.compile('<div class="box picblock col3".*?<img src2="(.*?)" alt="(.*?)">',re.S)
      # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
      pattern = re.compile('<div class="box picblock col3".*?src2="(.*?)" alt="(.*?)"></a>',re.S)
      objs = pattern.findall(content)
      # éå†
      for obj in objs:
          # åç¼€å
          suffix = obj[0].split('.')[-1]
          # æ‹¼æ¥æ–‡ä»¶å
          filename = './qinglv/'+obj[1]+'.'+suffix
          # ä¸‹è½½
          urllib.request.urlretrieve(url=obj[0],filename=filename)
  
  if __name__ == '__main__':
      start_page =int(input('è¯·è¾“å…¥èµ·å§‹é¡µç '))
      end_page = int(input('è¯·è¾“å…¥ç»“æŸé¡µç '))
      for page in range(start_page,end_page + 1):
          request = create_request(page)
          content = get_content(request)
          down_load(content)
  ```

### 3. xpath

+ xpathçš„ä½¿ç”¨		

  ```python
  1.å®‰è£…lxmlåº“      
  	pip install lxml 
  2.å¯¼å…¥lxml.etree  
      from lxml import etree
  3.etree.parse()   è§£ææœ¬åœ°æ–‡ä»¶
      html_tree = etree.parse('XX.html')	
  4.etree.HTML()    æœåŠ¡å™¨å“åº”æ–‡ä»¶
      html_tree = etree.HTML(response.read().decode('utf-8')	
  5.html_tree.xpath(xpathè·¯å¾„)
  ```

+ xpathåŸºæœ¬è¯­æ³•

  ```python
  1.è·¯å¾„æŸ¥è¯¢
  	//ï¼šæŸ¥æ‰¾æ‰€æœ‰å­å­™èŠ‚ç‚¹ï¼Œä¸è€ƒè™‘å±‚çº§å…³ç³»
  	/ ï¼šæ‰¾ç›´æ¥å­èŠ‚ç‚¹
  2.è°“è¯æŸ¥è¯¢
  	//div[@id]  
  	//div[@id="maincontent"]    
  3.å±æ€§æŸ¥è¯¢
  	//@class         
  4.æ¨¡ç³ŠæŸ¥è¯¢
  	//div[contains(@id, "he")]   
  	//div[starts-with(@id, "he")] 
  5.å†…å®¹æŸ¥è¯¢
  	//div/h1/text()
  6.é€»è¾‘è¿ç®—
  	//div[@id="head" and @class="s_down"]
  	//title | //price
  ```

### 4. xpathè§£æ

+ xpathæœ¬åœ°è§£æ

  ```python
  from lxml import etree
  
  # æœ¬åœ°æ–‡ä»¶ä¸­å¿…é¡»æ‰€æœ‰çš„æ ‡ç­¾éƒ½æœ‰ç»“å°¾
  # xpathæœ¬åœ°è§£æ
  tree = etree.parse('index.html')
  #       //ï¼šæŸ¥æ‰¾æ‰€æœ‰å­å­™èŠ‚ç‚¹ï¼Œä¸è€ƒè™‘å±‚çº§å…³ç³»
  # 		/ ï¼šæ‰¾ç›´æ¥å­èŠ‚ç‚¹
  a = tree.xpath('//div[@id="d1"]//li')
  print(a)
  
  # æ‰¾åˆ°divæ ‡ç­¾ä¸­æœ‰idå±æ€§çš„
  a = tree.xpath('//div[@id]')
  print(a)
  
  # å±æ€§æŸ¥è¯¢
  a = tree.xpath('//div[@id="d1"]/ul/li[1]/@class')
  print(a)
  
  a_list = tree.xpath('//div[@class="d2"]/ul/li[contains(@id,"l")]/@id')
  for a in a_list:
      print(a)
  
  a = tree.xpath('//div[@class="d2"]/ul/li[starts-with(@id,"c")]/@id')
  print(a)
  
  a = tree.xpath('//div[@id="d1"]/a/text()')
  print(a)
  # --------------ä¸‹é¢æ˜¯HTMLç½‘é¡µä»£ç ---------------------
  <!DOCTYPE html>
  <html lang="en">
  <head>
      <meta charset="UTF-8"/>
      <title>Title</title>
  </head>
  <body>
      <div id="d1">
          <ul>
              <li class="l1">hen</li>
              <li>dog</li>
              <li>cat</li>
              <li>pig</li>
              <li>monky</li>
          </ul>
          <a href="http://www.baidu.com">zoo</a>
      </div>
      
      <div class="d2">
          <ul>
              <li id="l1">å¤§è¿</li>
              <li id="l2">é”¦å·</li>
              <li id="c1">æˆéƒ½</li>
              <li>å¤§ç†</li>
              <li>æ—¥æœ¬</li>
          </ul>
          <a href="http://www.tuniu.com">location</a>
      </div>
  </body>
  </html>
  ```

+ xpathåœ¨çº¿è§£æâ€”â€”ç™¾åº¦(å°æ ‡é¢˜)

  ```python
  import urllib.request
  
  url = 'http://www.baidu.com'
  # è¯·æ±‚å¤´
  headers={
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
  }
  # è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
  request = urllib.request.Request(url=url,headers=headers)
  # æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
  response = urllib.request.urlopen(request)
  # è¯»å–æœåŠ¡å™¨å“åº”çš„æ•°æ®
  content = response.read().decode('utf-8')
  
  from lxml import etree
  # xpathçº¿ä¸Šè§£æ
  tree = etree.HTML(content)
  a_list = tree.xpath('//div[@id="u1"]/a/text()')
  # éå†
  for a in a_list:
      print(a)
  ```

+ xpathåœ¨çº¿è§£æâ€”â€”ç«™é•¿ç´ æ(å›¾ç‰‡)

  ```python
  import urllib.request
  
  # è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
  def create_request(page):
      base_url = 'http://sc.chinaz.com/tupian/qinglvtupian'
      if page == 1:
          url = base_url + '.html'
      else:
          url = base_url + '_' + str(page) + '.html'
  	# è¯·æ±‚å¤´
      headers = {
          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
  }
      # è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
      request = urllib.request.Request(url=url,headers=headers)
      return request
  
  # è·å–é¡µé¢çš„æºç 
  def get_content(request):
      # æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
      response = urllib.request.urlopen(request)
      # è¯»å–æœåŠ¡å™¨å“åº”çš„æ•°æ®
      content = response.read().decode('utf-8')
      return content
  
  from lxml import etree
  # è¯»å–çš„æ•°æ®å†™å…¥åˆ°æ–‡ä»¶ä¸­
  def down_load(content):
      # xpathåœ¨çº¿è§£æ
      tree = etree.HTML(content)
      # æŸ¥æ‰¾æ‰€éœ€çš„æ•°æ®
      src_list = tree.xpath('//div[@id="container"]//a/img/@src2')
      alt_list = tree.xpath('//div[@id="container"]//a/img/@alt')
      for i in range(len(src_list)):
          src = src_list[i]
          alt = alt_list[i]
          filename = './qinglv/'+alt + '.jpg'
          urllib.request.urlretrieve(url=src,filename=filename)
  
  if __name__ == '__main__':
      start_page = int(input('è¯·è¾“å…¥èµ·å§‹é¡µç '))
      end_page = int(input('è¯·è¾“å…¥ç»“æŸé¡µç '))
  
      for page in range(start_page,end_page+1):
          request = create_request(page)
          content = get_content(request)
          down_load(content)
  ```

### 5. jsonpath

+ jsonpathçš„ä½¿ç”¨

  ```python
  1.å®‰è£…ï¼š 
  	pip install jsonpath
  2.jsonpathçš„ä½¿ç”¨ï¼š
  	obj = json.load(open('jsonæ–‡ä»¶', 'r', encoding='utf-8'))
  	ret = jsonpath.jsonpath(obj, 'jsonpathè¯­æ³•')
  ```

+ jsonpathåŸºæœ¬è¯­è¨€

  | Xpath | JSONPath         | Description                                                  |
  | ----- | ---------------- | ------------------------------------------------------------ |
  | /     | $                | è¡¨ç¤ºæ ¹å…ƒç´                                                    |
  | .     | @                | å½“å‰å…ƒç´                                                      |
  | /     | . or []          | å­å…ƒç´                                                        |
  | ..    | n/a              | çˆ¶å…ƒç´                                                        |
  | //    | ..               | é€’å½’ä¸‹é™ï¼ŒJSONPathæ˜¯ä»E4Xå€Ÿé‰´çš„ã€‚                            |
  | *     | *                | é€šé…ç¬¦ï¼Œè¡¨ç¤ºæ‰€æœ‰çš„å…ƒç´                                        |
  | @     | n/a              | å±æ€§è®¿é—®å­—ç¬¦                                                 |
  | []    | []               | å­å…ƒç´ æ“ä½œç¬¦                                                 |
  | \|    | [,]              | è¿æ¥æ“ä½œç¬¦åœ¨XPath ç»“æœåˆå¹¶å…¶å®ƒç»“ç‚¹é›†åˆã€‚JSONPå…è®¸nameæˆ–è€…æ•°ç»„ç´¢å¼•ã€‚ |
  | n/a   | [start:end:step] | æ•°ç»„åˆ†å‰²æ“ä½œä»ES4å€Ÿé‰´ã€‚                                      |
  | []    | ?()              | åº”ç”¨è¿‡æ»¤è¡¨ç¤ºå¼                                               |
  | n/a   | ()               | è„šæœ¬è¡¨è¾¾å¼ï¼Œä½¿ç”¨åœ¨è„šæœ¬å¼•æ“ä¸‹é¢ã€‚                             |
  | ()    | n/a              | Xpathåˆ†ç»„                                                    |

+ jsonå¯¹è±¡çš„è½¬æ¢

  ```python
  1.json.loads()	æ˜¯å°†å­—ç¬¦ä¸²è½¬åŒ–ä¸ºpythonå¯¹è±¡
  2.json.dumps()	å°†pythonå¯¹è±¡è½¬åŒ–ä¸ºjsonæ ¼å¼çš„å­—ç¬¦ä¸²
  3.json.load()	è¯»å–jsonæ ¼å¼çš„æ–‡æœ¬ï¼Œè½¬åŒ–ä¸ºpythonå¯¹è±¡
  	json.load(open(a.json))	
  4.json.dump()	å°†pythonå¯¹è±¡å†™å…¥åˆ°æ–‡æœ¬ä¸­
  ```

### 6. jsonpathè§£æ

+ jsonpathçš„åŸºæœ¬ä½¿ç”¨

  ```python
  import jsonpath
  import json
  
  obj = json.load(open('store.json','r',encoding='utf-8'))
  
  # ä¹¦åº—æ‰€æœ‰ä¹¦çš„ä½œè€…
  author_list = jsonpath.jsonpath(obj,'$.store.book[*].author')
  for author in author_list:
      print(author)
  
  
  # æ‰€æœ‰çš„ä½œè€…
  author_list = jsonpath.jsonpath(obj,'$..author')
  for author in author_list:
      print(author)
  
  # storeçš„æ‰€æœ‰å…ƒç´ ã€‚æ‰€æœ‰çš„bookså’Œbicycle
  a_list = jsonpath.jsonpath(obj,'$.store.*')
  
  # storeé‡Œé¢æ‰€æœ‰ä¸œè¥¿çš„price
  price_list = jsonpath.jsonpath(obj,'$.store..price')
  	for price in price_list:
       	print(price)
  
  #
  # ç¬¬ä¸‰ä¸ªä¹¦
  book = jsonpath.jsonpath(obj,'$..book[2]')
  print(book)
  
  # æœ€åä¸€æœ¬ä¹¦
  book = jsonpath.jsonpath(obj,'$..book[(@.length-1)]')
  print(book)
  
  # 	å‰é¢çš„ä¸¤æœ¬ä¹¦ã€‚
  book_list = jsonpath.jsonpath(obj,'$..book[0,1]')
  print(book_list)
  book_list = jsonpath.jsonpath(obj,'$..book[:2]')
  print(book_list)
  
  # è¿‡æ»¤å‡ºæ‰€æœ‰çš„åŒ…å«isbnçš„ä¹¦ã€‚
  book_list = jsonpath.jsonpath(obj,'$..book[?(@.isbn)]')
  print(book_list)
  
  # 	è¿‡æ»¤å‡ºä»·æ ¼ä½äº10çš„ä¹¦
  book_list = jsonpath.jsonpath(obj,'$..book[?(@.price < 10)]')
  print(book_list)
  
  # ---------------------ä»¥ä¸‹æ˜¯è¦JSONæ•°æ®-----------------
  { "store": {
      "book": [
        { "category": "ç„å¹»",
          "author": "å¤©èš•åœŸè±†",
          "title": "æ–—ç ´è‹ç©¹",
          "price": 8.95
        },
        { "category": "ç„å¹»",
          "author": "å”å®¶ä¸‰å°‘",
          "title": "æ–—ç½—å¤§é™†",
          "price": 12.99
        },
        { "category": "è¨€æƒ…",
          "author": "æˆ‘åƒè¥¿çº¢æŸ¿",
          "title": "æ˜Ÿè¾°å˜",
          "isbn": "0-553-21311-3",
          "price": 8.99
        },
        { "category": "è¨€æƒ…",
          "author": "å‰å¤š",
          "title": "pythonä»å…¥é—¨åˆ°æ”¾å¼ƒ",
          "isbn": "0-395-19395-8",
          "price": 22.99
        }
      ],
      "bicycle": {
        "color": "red",
        "price": 19.95
      }
    }
  }
  ```

+ jsonpathè§£æâ€”â€”æ™ºè”æ‹›è˜

  ```python
  import jsonpath
  import json
  import urllib.request
  
  # APIæ¥å£
  url = 'https://fe-api.zhaopin.com/c/i/sou?pageSize=90&cityId=538&workExperience=-1&education=-1&companyType=-1&employmentType=-1&jobWelfareTag=-1&kw=python&kt=3&_v=0.45173956&x-zp-page-request-id=8e8e88b85c494aac8ab3533ebc6ddd5a-1571908246825-8289&x-zp-client-id=94df40c0-69a5-40f2-9d66-0bbe28f523a4'
  # è¯·æ±‚å¤´
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
  }
  # è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
  request = urllib.request.Request(url=url,headers=headers)
  # æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚ 
  response = urllib.request.urlopen(request)
  # è¯»å–æœåŠ¡å™¨å“åº”çš„æ•°æ®
  content = response.read().decode('utf-8')
  # è¯»å–çš„æ•°æ®å†™å…¥åˆ°æ–‡ä»¶ä¸­
  with open('zhilian.json','w',encoding='utf-8')as fp:
      fp.write(content)
  # jsonpathæœ¬åœ°è§£æ(è¯»å–ä»ç½‘é¡µä¿å­˜åˆ°æœ¬åœ°çš„æ•°æ®)
  obj = json.load(open('zhilian.json','r',encoding='utf-8'))
  # è§£æè‡ªå·±æ‰€éœ€çš„æ•°æ®
  jobname_list = jsonpath.jsonpath(obj,'$..jobName')
  companyname_list = jsonpath.jsonpath(obj,'$..company.name')
  salary_list = jsonpath.jsonpath(obj,'$..salary')
  
  jobInfo_list = []
  
  for i in range(len(jobname_list)):
      job = jobname_list[i]
      cname = companyname_list[i]
      salary = salary_list[i]
      jobInfo = {}
      jobInfo['job']=job
      jobInfo['cname']=cname
      jobInfo['salary']=salary
      jobInfo_list.append(jobInfo)
  # ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ä¸­
  with open('zhilian1.json','w',encoding='utf-8')as fp:
      fp.write(str(jobInfo_list))
  ```


### 7. BeautifulSoup

+ BeautifulSoupåŸºæœ¬ç®€ä»‹â€”â€”bs4

  ```python
  1.BeautifulSoupç®€ç§°ï¼š
  	bs4
  2.ä»€ä¹ˆæ˜¯BeatifulSoupï¼Ÿ
  	BeautifulSoupï¼Œå’Œlxmlä¸€æ ·ï¼Œæ˜¯ä¸€ä¸ªhtmlçš„è§£æå™¨ï¼Œä¸»è¦åŠŸèƒ½ä¹Ÿæ˜¯è§£æå’Œæå–æ•°æ®
  3.ä¼˜ç¼ºç‚¹ï¼Ÿ
  	ç¼ºç‚¹ï¼šæ•ˆç‡æ²¡æœ‰lxmlçš„æ•ˆç‡é«˜	
  	ä¼˜ç‚¹ï¼šæ¥å£è®¾è®¡äººæ€§åŒ–ï¼Œä½¿ç”¨æ–¹ä¾¿
  ```

+ BeautifulSoupå®‰è£…å’Œåˆ›å»º

  ```python
  1.å®‰è£…
  	pip install bs4
  2.å¯¼å…¥
  	from bs4 import BeautifulSoup
  3.åˆ›å»ºå¯¹è±¡
  	1.æœ¬åœ°æ–‡ä»¶ç”Ÿæˆå¯¹è±¡
  		soup = BeautifulSoup(open('1.html'), 'lxml')
  		æ³¨æ„ï¼šé»˜è®¤æ‰“å¼€æ–‡ä»¶çš„ç¼–ç æ ¼å¼gbkæ‰€ä»¥éœ€è¦æŒ‡å®šæ‰“å¼€ç¼–ç æ ¼å¼	
  	2.æœåŠ¡å™¨å“åº”çš„æ–‡ä»¶ç”Ÿæˆå¯¹è±¡
  		soup = BeautifulSoup(response.read().decode(), 'lxml')
  ```

+ BeautifulSoupåŸºç¡€çŸ¥è¯†â€”â€”èŠ‚ç‚¹å®šä½

  ```python
  1.æ ¹æ®æ ‡ç­¾åæŸ¥æ‰¾èŠ‚ç‚¹
  	soup.a ã€æ³¨ã€‘åªèƒ½æ‰¾åˆ°ç¬¬ä¸€ä¸ªa
  		soup.a.name
  		soup.a.attrs
  2.å‡½æ•°
  	1.find(è¿”å›ä¸€ä¸ªå¯¹è±¡)
      	find('a')ï¼šåªæ‰¾åˆ°ç¬¬ä¸€ä¸ªaæ ‡ç­¾
          find('a', title='åå­—')
          find('a', class_='åå­—')
  	2.find_all(è¿”å›ä¸€ä¸ªåˆ—è¡¨)
      	find_all('a')  æŸ¥æ‰¾åˆ°æ‰€æœ‰çš„a
          find_all(['a', 'span'])  è¿”å›æ‰€æœ‰çš„aå’Œspan
          find_all('a', limit=2)  åªæ‰¾å‰ä¸¤ä¸ªa
  	3.select(æ ¹æ®é€‰æ‹©å™¨å¾—åˆ°èŠ‚ç‚¹å¯¹è±¡)ã€æ¨èã€‘
          1.element
          	eg:p
          2..class
              eg:.firstname
          3.#id
              eg:#firstname
          4.å±æ€§é€‰æ‹©å™¨
              [attribute]
              	eg:li = soup.select('li[class]')
              [attribute=value]
              	eg:li = soup.select('li[class="hengheng1"]')
          5.å±‚çº§é€‰æ‹©å™¨
              1.åä»£é€‰æ‹©å™¨div p
              2.å­ä»£é€‰æ‹©å™¨div>p
              3.ç¾¤ç»„é€‰æ‹©å™¨div,p 
              	eg:soup = soup.select('a,span')
  3.è·å–å­å­™èŠ‚ç‚¹
  	contentsï¼šè¿”å›çš„æ˜¯ä¸€ä¸ªåˆ—è¡¨
  		eg:print(soup.body.contents)
      descendantsï¼šè¿”å›çš„æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨
     		eg:for a in soup.body.descendants:
          	print(a)
  ```

+ BeautifulSoupåŸºç¡€çŸ¥è¯†â€”â€”èŠ‚ç‚¹ä¿¡æ¯

  ```
  1.è·å–èŠ‚ç‚¹å†…å®¹ï¼šé€‚ç”¨äºæ ‡ç­¾ä¸­åµŒå¥—æ ‡ç­¾çš„ç»“æ„
  	obj.string
  	obj.get_text()ã€æ¨èã€‘
  2.èŠ‚ç‚¹çš„å±æ€§
  	tag.name è·å–æ ‡ç­¾å
  		eg: tag = find('li)
  			print(tag.name)
  	tag.attrså°†å±æ€§å€¼ä½œä¸ºä¸€ä¸ªå­—å…¸è¿”å›
  (3).è·å–èŠ‚ç‚¹å±æ€§
  	obj.attrs.get('title')ã€å¸¸ç”¨ã€‘
  	obj.get('title')
  	obj['title']
  ```

+ BeautifulSoupåŸºç¡€çŸ¥è¯†â€”â€”èŠ‚ç‚¹ç±»å‹ï¼ˆç†è§£ï¼‰

  ```python
  bs4.BeautifulSoup æ ¹èŠ‚ç‚¹ç±»å‹
  bs4.element.NavigableString è¿æ¥ç±»å‹  å¯æ‰§è¡Œçš„å­—ç¬¦ä¸²
  bs4.element.Tag èŠ‚ç‚¹ç±»å‹  
  bs4.element.Comment æ³¨é‡Šç±»å‹
  	eg:
  		if type(aobj.string) == bs4.element.Comment:
          	print('è¿™ä¸ªæ˜¯æ³¨é‡Šå†…å®¹')
          else:
              print('è¿™ä¸æ˜¯æ³¨é‡Š')
  ```

### 8. BeautifulSoupè§£æ

+ bs4çš„åŸºæœ¬è¯­æ³•ä½¿ç”¨

  ```python
  from bs4 import BeautifulSoup
  
  # åŠ è½½çš„æ˜¯æœ¬åœ°æ–‡ä»¶  bs4é»˜è®¤æ‰“å¼€æ–‡ä»¶çš„ç¼–ç æ˜¯gbk
  soup = BeautifulSoup(open('index.html',encoding='utf-8'),'lxml')
  # æ ¹æ®æ ‡ç­¾åæŸ¥æ‰¾èŠ‚ç‚¹åªèƒ½æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ ‡ç­¾
  print(soup.a)
  
  # find æ–¹æ³•è¿”å›çš„æ˜¯ä¸€ä¸ªå¯¹è±¡
  # æŸ¥æ‰¾åˆ°ç¬¬ä¸€ä¸ªaæ ‡ç­¾
  a = soup.find('a')
  print(a)
  # æ ¹æ®æ ‡ç­¾çš„å±æ€§titleæ¥æŸ¥æ‰¾æ ‡ç­¾
  a = soup.find('a',title='qf')
  print(a)
  # ä¸å¯ä»¥å®ç°
  # a = soup.find('a',name='fq')
  # print(a)
  # æ ¹æ®æ ‡ç­¾çš„å±æ€§idæ¥æŸ¥æ‰¾æ ‡ç­¾
  a = soup.find('a',id='qf1')
  print(a)
  # æ ¹æ®æ ‡ç­¾çš„å±æ€§class_æ¥æŸ¥æ‰¾æ ‡ç­¾--classæ˜¯å…³é”®å­—æ‰€ä»¥è¦åŠ ä¸‹åˆ’çº¿
  a = soup.find('a',class_='bd')
  print(a)
  # æ ¹æ®æ ‡ç­¾çš„è‡ªå®šä¹‰çš„å±æ€§bdæ¥æŸ¥æ‰¾æ ‡ç­¾
  a = soup.find('a',bd='bd1')
  print(a)
  
  # findallæ–¹æ³•è¿”å›ä¸€ä¸ªåˆ—è¡¨
  # è¿”å›æ‰€æœ‰çš„aæ ‡ç­¾
  a_list = soup.find_all('a')
  print(a_list)
  # è¿”å›æ‰€æœ‰çš„aæ ‡ç­¾å’Œspanæ ‡ç­¾
  a_list = soup.find_all(['a','span'])
  print(a_list)
  # è¿”å›å‰ä¸¤ä¸ªliæ ‡ç­¾
  li_list = soup.find_all('li',limit=2)
  print(li_list)
  
  # select æ ¹æ®é€‰æ‹©å™¨å¾—åˆ°èŠ‚ç‚¹å¯¹è±¡
  # é€šè¿‡æ ‡ç­¾é€‰æ‹©å™¨aæ¥è·å–aæ ‡ç­¾
  a = soup.select('a')
  print(a)
  # é€šè¿‡ç±»é€‰æ‹©å™¨æ¥è·å–liæ ‡ç­¾
  li = soup.select('.l1')
  print(li)
  # é€šè¿‡idé€‰æ‹©å™¨æ¥è·å–
  li = soup.select('#l2')
  print(li)
  # é€šè¿‡æ ‡ç­¾å±æ€§é€‰æ‹©å™¨æ¥è·å–
  li = soup.select('li[class]')
  print(li)
  # é€šè¿‡æ ‡ç­¾å±æ€§é€‰æ‹©å™¨æ¥è·å–
  li = soup.select('li[class="l1"]')
  print(li)
  # é€šè¿‡åä»£é€‰æ‹©å™¨æ¥è·å–
  li_list = soup.select('div li')
  print(li_list)
  # é€šè¿‡åä»£é€‰æ‹©å™¨æ¥è·å–(æ³¨æ„ï¼šç©ºæ ¼)
  li = soup.select('span > span')
  print(li)
  # é€šè¿‡ç¾¤ç»„é€‰æ‹©å™¨è·å–aæˆ–è€…spanæ ‡ç­¾
  list = soup.select('a,span')
  print(list)
  
  # è·å–æŸæ ‡ç­¾ä¸‹çš„æ‰€æœ‰çš„å­èŠ‚ç‚¹
  print(soup.body.contents)
  # éå†(ä»¥ç”Ÿæˆå™¨å½¢å¼éå†)
  for a in soup.body.descendants:
     print(a)
  
  # è·å–èŠ‚ç‚¹å†…å®¹
  s = soup.select('#s1')[0]
  print(type(s))
  # <span id="s1">
  #         <span>è¿˜æœ‰ä¸€å‘¨æˆ‘ä»¬çˆ¬è™«å°±ç»“æŸäº†</span>
  # </span>
  # stringæƒ³è¦è·å– é‚£ä¹ˆè¯¥å¯¹è±¡çš„æ ‡ç­¾å†… ä¸å…è®¸åœ¨å­˜åœ¨æ ‡ç­¾
  print(s.string)
  # get_text()è·å–æ ‡ç­¾å†…çš„æ–‡æœ¬
  print(s.get_text())
  
  # nameå±æ€§è·å–çš„æ˜¯æ ‡ç­¾çš„åå­—
  s = soup.select('#s1')[0]
  print(s.name)
  
  # è·å–èŠ‚ç‚¹çš„å±æ€§
  p = soup.select('#p1')[0]
  print(p.attrs)
  print(p.attrs.get('class'))
  print(p.get('name'))
  print(p['id'])
  # -------------------ä¸‹é¢æ˜¯HTML------------------
  <!DOCTYPE html>
  <html lang="en">
  <head>
      <meta charset="UTF-8">
      <title>Title</title>
  </head>
  <body>
      <p id="p1" class="p2" name="p3"></p>
      <span id="s1">
          <span>è¿˜æœ‰ä¸€å‘¨æˆ‘ä»¬çˆ¬è™«å°±ç»“æŸäº†</span>
          æˆ‘ä»¬å°±å†è§äº†
      </span>
      <div>
          <ul>
              <li class="l1">é…¸èœç‚–æ’éª¨</li>
              <li id="l2">å¹²è±†è…å·å¤§è‘±</li>
              <li>å®æ³¢å†å›å¤´ç‰›è›™</li>
              <li>å‘¨é»‘é¸­+å•¤é…’</li>
              <li>ç‚’é¸¡è›‹</li>
          </ul>
          <a href="http://www.baidu.com" class="bd" bd="bd1">ç™¾åº¦</a>
      </div>
      <div>
          <ul>
              <li>æ¡‚æ—</li>
              <li>é©¬å°”ä»£å¤«</li>
              <li>åœŸè€³å…¶</li>
              <li>å¼ å®¶å ¡æ‘</li>
              <li>å¤§è¿</li>
          </ul>
          <span>å½“å½“</span>
          <a href="http://www.1000phone.com" title="qf" name="fq" id="qf1">åƒé”‹</a>
      </div>
  </body>
  </html>
  ```

+ bs4è§£æâ€”â€”è‚¡ç¥¨ç½‘

  ```python
  import urllib.request
  from bs4 import BeautifulSoup
  # è¯·æ±‚å¯¹è±¡å®šåˆ¶
  def create_request():
      # url
      url = 'http://quote.stockstar.com/'
      # è¯·æ±‚å¤´
      headers = {
          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
      }
      # è¯·æ±‚å¯¹è±¡å®šåˆ¶
      request = urllib.request.Request(url=url,headers=headers)
      return request
  
  # è·å–æœåŠ¡å™¨å“åº”çš„æ•°æ®
  def get_content(request):
      # æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€æ•°æ®
      response = urllib.request.urlopen(request)
      # è·å–æœåŠ¡å™¨å“åº”çš„æ•°æ®
      content = response.read().decode('gb2312')
      # print(content)
      return content
  
  # æŠŠè·å–çš„æ•°æ®ä¿å­˜åˆ°æ–‡ä»¶ä¸­
  def down_load(content):
      # ç”¨bs4è§£æ
      content = BeautifulSoup(content,'lxml')
      # é€šè¿‡è§£æè·å–è‡ªå·±æƒ³è¦çš„æ•°æ®
      num_name_list = content.select('#datalist a')
      price_list = content.select('#datalist span')
      data = {}
      for i in range(int(len(num_name_list)/2)):
          data['num'] = num_name_list[2*i].get_text()
          data['name'] = num_name_list[2*i+1].get_text()
          data['price'] = price_list[2*i].get_text()
          print(data)
  
  if __name__ == '__main__':
      request = create_request()
      content = get_content(request)
      down_load(content)
  ```


## 4. selenium

### 1. selenium å®‰è£…

```python
1.æ“ä½œè°·æ­Œæµè§ˆå™¨é©±åŠ¨ä¸‹è½½åœ°å€
	http://chromedriver.storage.googleapis.com/index.html 
2.è°·æ­Œé©±åŠ¨å’Œè°·æ­Œæµè§ˆå™¨ç‰ˆæœ¬ä¹‹é—´çš„æ˜ å°„è¡¨
	http://blog.csdn.net/huilan_same/article/details/51896672
3.æŸ¥çœ‹è°·æ­Œæµè§ˆå™¨ç‰ˆæœ¬
	è°·æ­Œæµè§ˆå™¨å³ä¸Šè§’-->å¸®åŠ©-->å…³äº
4.å®‰è£…selenium
	pip install selenium
```

### 2. seleniumçš„ä½¿ç”¨

+ seleniumçš„çŸ¥è¯†ç‚¹

  ```python
  1.å¯¼åŒ…ï¼š
  	from selenium import webdriver
  2.åˆ›å»ºè°·æ­Œæµè§ˆå™¨æ“ä½œå¯¹è±¡ï¼š
  	path = è°·æ­Œæµè§ˆå™¨é©±åŠ¨æ–‡ä»¶è·¯å¾„
  	browser = webdriver.Chrome(path)
  3.è®¿é—®ç½‘å€
  	url = è¦è®¿é—®çš„ç½‘å€
  	browser.get(url)
  ```

+ seleniumçš„åŸºæœ¬ä½¿ç”¨

  ```python
  from selenium import webdriver
  import time
  # è°·æ­Œæµè§ˆå™¨é©±åŠ¨æ–‡ä»¶çš„è·¯å¾„
  path = 'chromedriver.exe'
  # åˆ›å»ºè°·æ­Œæµè§ˆå™¨æ“ä½œå¯¹è±¡
  browser = webdriver.Chrome(path)
  # è®¿é—®çš„ç½‘å€(è®¿é—®çš„è·¯ç”±)
  url = 'http://www.baidu.com'
  # è®¿é—®æµè§ˆå™¨
  browser.get(url=url)
  # å»¶è¿Ÿ3s
  time.sleep(3)
  # é€€å‡ºæµè§ˆå™¨
  browser.quit()
  ```

### 3.  seleniumå…ƒç´ å®šä½ä¸äº¤äº’

+ seleniumå…ƒç´ çš„å®šä½

  ```python
  1.find_element_by_id			# é€šè¿‡idæ¥å®šä½å…ƒç´ 
  2.find_elements_by_name			# é€šè¿‡classå±æ€§æ¥å®šä½å…ƒç´ 
  3.find_elements_by_xpath		# é€šè¿‡xpathæ¥å®šä½å…ƒç´ 
  4.find_elements_by_tag_name 	# é€šè¿‡æ ‡ç­¾æ¥å®šä½å…ƒç´ 
  5.find_elements_by_css_selector	# é€šè¿‡bs4æ¥å®šä½å…ƒç´ 
  6.find_elements_by_link_text	# é€šè¿‡aæ ‡ç­¾çš„æ–‡æœ¬æ¥å®šä½å…ƒç´ 
  ```

+ è®¿é—®å…ƒç´ ä¿¡æ¯

  ```python
  .get_attribute('class')		# è·å–å…ƒç´ å±æ€§
  .text						# è·å–å…ƒç´ æ–‡æœ¬
  .id							# è·å–id
  .tag_name					# è·å–æ ‡ç­¾å
  ```

+ äº¤äº’

  ```python
  ç‚¹å‡»:click()
  è¾“å…¥:send_keys()
  åé€€æ“ä½œ:browser.back()
  å‰è¿›æ“ä½œ:browser.forword()
  æ¨¡æ‹ŸJSæ»šåŠ¨:
  	js = 'document.body.scrollTop=100000' # æˆ–åˆ™ä¸‹é¢ä¸€å¥
  	js='document.documentElement.scrollTop=100000'
  	browser.execute_script(js) æ‰§è¡Œjsä»£ç 
  è·å–ç½‘é¡µä»£ç ï¼špage_source 
  é€€å‡ºï¼šbrowser.quit()
  ```

+ seleniumå…ƒç´ å®šä½ä¸äº¤äº’çš„åŸºæœ¬ä½¿ç”¨

  ```python
  from selenium import webdriver
  import time
  # è°·æ­Œæµè§ˆå™¨é©±åŠ¨æ–‡ä»¶çš„è·¯å¾„
  path = 'chromedriver.exe'
  # åˆ›å»ºè°·æ­Œæµè§ˆå™¨æ“ä½œå¯¹è±¡
  browser = webdriver.Chrome(path)
  # è®¿é—®çš„ç½‘å€(è®¿é—®çš„è·¯ç”±)
  url = 'http://www.baidu.com'
  # è®¿é—®æµè§ˆå™¨
  browser.get(url=url)
  # å»¶è¿Ÿ2s
  time.sleep(2)
  # é€šè¿‡idæ¥å®šä½inputè¿™ä¸ªæ ‡ç­¾
  t = browser.find_element_by_id('kw')
  # å‘é‡Œé¢è¾“å…¥'éŸ©çº¢'
  t.send_keys('éŸ©çº¢')
  # å»¶è¿Ÿ3s
  time.sleep(3)
  # é€šè¿‡idæ¥å®šä½'ç™¾åº¦ä¸€ä¸‹'æ ‡ç­¾
  su = browser.find_element_by_id('su')
  # ç‚¹å‡»
  su.click()
  # å»¶è¿Ÿ3s
  time.sleep(3)
  # è¿”å›ä¸Šä¸€ä¸ªé¡µé¢
  browser.back()
  # å»¶è¿Ÿ3s
  time.sleep(3)
  # å‘å‰æ“ä½œä¸€æ­¥
  browser.forward()
  # å»¶è¿Ÿ2s
  time.sleep(2)
  # é€€å‡º
  browser.quit()
  ```

### 5. Phantomjs

+ phantomjsçš„sä½¿ç”¨æ­¥éª¤

  ```python
  1.è·å–PhantomJS.exeæ–‡ä»¶è·¯å¾„path
  2.browser = webdriver.PhantomJS(path)
  3.browser.get(url)
  æ‰©å±•ï¼šä¿å­˜å±å¹•å¿«ç…§:browser.save_screenshot('baidu.png')
  ```

+ phantomjsçš„åŸºæœ¬ä½¿ç”¨

  ```python
  from selenium import webdriver
  import time
  # è°·æ­Œæµè§ˆå™¨é©±åŠ¨æ–‡ä»¶çš„è·¯å¾„
  path = 'phantomjs.exe'
  # åˆ›å»ºè°·æ­Œæµè§ˆå™¨æ“ä½œå¯¹è±¡
  browser = webdriver.PhantomJS(path)
  # è®¿é—®çš„ç½‘å€(è®¿é—®çš„è·¯ç”±)
  url = 'http://www.baidu.com'
  # è®¿é—®æµè§ˆå™¨
  browser.get(url=url)
  # ä¿å­˜å±å¹•å¿«ç…§
  browser.save_screenshot('baidu.png')
  # å»¶è¿Ÿ2s
  time.sleep(2)
  # é€šè¿‡idæ¥å®šä½inputè¿™ä¸ªæ ‡ç­¾
  t = browser.find_element_by_id('kw')
  # å‘é‡Œé¢è¾“å…¥'éŸ©çº¢'
  t.send_keys('éŸ©çº¢')
  # ä¿å­˜å±å¹•å¿«ç…§
  browser.save_screenshot('baidu1.png')
  # é€šè¿‡idæ¥å®šä½'ç™¾åº¦ä¸€ä¸‹'æ ‡ç­¾
  b = browser.find_element_by_id('su')
  # ç‚¹å‡»
  b.click()
  # ä¿å­˜å±å¹•å¿«ç…§
  browser.save_screenshot('baidu2.png')
  # å‘ä¸‹æ»‘åŠ¨åˆ°åº•
  js='document.documentElement.scrollTop=100000'
  # æ‰§è¡Œjsä»£ç 
  browser.execute_script(js)
  # ä¿å­˜å±å¹•å¿«ç…§
  browser.save_screenshot('baidu3.png')
  ```

### 6. Chrome handless

+ é…ç½®

  ```python
  from selenium import webdriver
  from selenium.webdriver.chrome.options import Options
  
  chrome_options = Options()
  chrome_options.add_argument('--headless')
  chrome_options.add_argument('--disable-gpu')
  
  # googleæµè§ˆå™¨çš„è·¯å¾„
  path = r'C:\Program Files (x86)\Google\Chrome\Application\chrome.exe'
  chrome_options.binary_location = path
  
  browser = webdriver.Chrome(chrome_options=chrome_options)
  browser.get('http://www.baidu.com/')
  browser.save_screenshot('baidu.png')
  ```

+ é…ç½®å°è£…

  ```python
  from selenium import webdriver
  # è¿™ä¸ªæ˜¯æµè§ˆå™¨è‡ªå¸¦çš„  ä¸éœ€è¦æˆ‘ä»¬å†åšé¢å¤–çš„æ“ä½œ
  from selenium.webdriver.chrome.options import Options
  
  def share_browser():
      # åˆå§‹åŒ–
      chrome_options = Options()
      chrome_options.add_argument('--headless')
      chrome_options.add_argument('--disable-gpu')
      # æµè§ˆå™¨çš„å®‰è£…è·¯å¾„    æ‰“å¼€æ–‡ä»¶ä½ç½®
      # è¿™ä¸ªè·¯å¾„æ˜¯ä½ è°·æ­Œæµè§ˆå™¨çš„è·¯å¾„
      path = r'C:\Program Files (x86)\Google\Chrome\Application\chrome.exe'
      chrome_options.binary_location = path
      browser = webdriver.Chrome(chrome_options=chrome_options)
      return browser
  # ----------------ä»¥ä¸‹æ˜¯å°è£…è°ƒç”¨-------------------------
  from handless import share_browser
  
  browser = share_browser()
  browser.get('http://www.baidu.com/')
  browser.save_screenshot('handless1.png')
  ```

## 5. request

### 1. requestçŸ¥è¯†ç‚¹

```
1.æ–‡æ¡£
	å®˜æ–¹æ–‡æ¡£:http://cn.python-requests.org/zh_CN/latest/
	å¿«é€Ÿä¸Šæ‰‹:http://cn.python-requests.org/zh_CN/latest/user/quickstart.html
2.å®‰è£…
	pip install request
```

### 2. getè¯·æ±‚

+ getè¯·æ±‚çŸ¥è¯†ç‚¹

  ```python
  å®šåˆ¶å‚æ•°
  	å‚æ•°ä½¿ç”¨paramsä¼ é€’
  	å‚æ•°æ— éœ€urlencode
  1. r.text : è·å–ç½‘ç«™æºç 
  2. r.encoding è®¿é—®æˆ–å®šåˆ¶ç¼–ç æ–¹å¼
  3. r.url è·å–è¯·æ±‚çš„url
  4. r.content å“åº”çš„å­—èŠ‚ç±»å‹
  5. r.status_code å“åº”çš„çŠ¶æ€ç 
  6. r.headers å“åº”çš„å¤´ä¿¡æ¯
  ```

+ getè¯·æ±‚çš„åŸºæœ¬ä½¿ç”¨

  ```python
  import requests
  # è®¿é—®çš„ç½‘å€(è®¿é—®çš„è·¯ç”±)
  url = 'http://www.baidu.com'
  # è®¿é—®æµè§ˆå™¨
  response = requests.get(url=url)
  # æŸ¥çœ‹responseçš„ç±»å‹
  print(type(response))
  # ä»¥äºŒè¿›åˆ¶æ ¼å¼æ¥è¯»å–æ–‡ä»¶
  print(response.content)
  # ä»¥å­—ç¬¦ä¸²æ ¼å¼æ¥è¯»å–æ–‡ä»¶
  print(response.text)
  # è·å–æœåŠ¡å™¨å“åº”çš„çŠ¶æ€ç 
  print(response.status_code)
  # è·å–è¯·æ±‚çš„url
  print(response.url)
  # è·å–å“åº”çš„å¤´ä¿¡æ¯
  print(response.headers)
  # å“åº”çš„æ—¶å€™è®¾ç½®ç¼–ç   æ‰“å°response.textçš„æ—¶å€™  é‚£ä¹ˆæœ‰ä¹±ç å‘ç”Ÿ é‚£ä¹ˆæˆ‘ä»¬
  # å¯ä»¥ä½¿ç”¨response.encodingæ¥è®¾ç½®ç¼–ç 
  response.encoding = 'utf-8'
  ```

+ getçš„ä¾‹å­

  ```python
  import requests
  # getè¯·æ±‚çš„ï¼Ÿå¯ä»¥åŠ ä¹Ÿå¯ä»¥ä¸åŠ 
  url = 'https://www.baidu.com/s'
  # å‚æ•°
  data = {
      'wd':'é™ˆå† å¸Œ'
  }
  # è¯·æ±‚å¤´
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36'
  }
  # è®¿é—®æµè§ˆå™¨
  response = requests.get(url=url,params=data,headers=headers)
  # è·å–ç½‘ç«™æºç 
  content = response.text
  print(content)
  ```

### 3. postè¯·æ±‚

+ postè¯·æ±‚çš„ä¾‹å­

  ```python
  import requests
  # è¦è®¿é—®çš„ç½‘å€
  url = 'https://fanyi.baidu.com/sug'
  # å‚æ•°
  data = {
      'kw': 'abandon'
  }
  # è¯·æ±‚å¤´
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36'
  }
  # è®¿é—®æµè§ˆå™¨
  response = requests.post(url=url,data=data,headers=headers)
  # è·å–ç½‘ç«™æºç 
  content = response.text
  
  # è§£å†³ä¸­æ–‡ä¹±ç 
  import json
  # å°†å­—ç¬¦ä¸²è½¬æ¢æˆjsonå¯¹è±¡
  obj = json.loads(content)
  # å°†jsonå¯¹è±¡è½¬æ¢æˆå­—ç¬¦ä¸²ï¼Œå¹¶å¯¹å…¶ç¼–ç 
  s = json.dumps(obj,ensure_ascii=False)
  print(s)
  ```

+ get  å’ŒpoståŒºåˆ«

  ```
  1ï¼šgetè¯·æ±‚çš„å‚æ•°åå­—æ˜¯params  postè¯·æ±‚çš„å‚æ•°çš„åå­—æ˜¯data  
  2 è¯·æ±‚èµ„æºè·¯å¾„åé¢å¯ä»¥ä¸åŠ ?  
  3 ä¸éœ€è¦æ‰‹åŠ¨ç¼–è§£ç   4 ä¸éœ€è¦åšè¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
  ```

### 4. proxyå®šåˆ¶

```python
import requests
# è¦è®¿é—®çš„ç½‘å€
url = 'http://www.baidu.com/s'
# å‚æ•°
data = {
    'wd':'ip'
}
# è¯·æ±‚å¤´
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36'
}
# IPä»£ç†
proxies = {
    'http':'114.239.147.135:9999'
}
# è®¿é—®æµè§ˆå™¨
response = requests.get(url=url,params=data,headers=headers,proxies=proxies)
# è·å–ç½‘ç«™æºç 
content = response.text
# ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ä¸­
with open('dl.html','w',encoding='utf-8')as fp:
    fp.write(content)
```

### 5. cookieå®šåˆ¶

+ ç¬‘è¯é›†

  ```python
  import requests
  # è¦è®¿é—®çš„ç½‘å€
  url_get = 'http://www.jokeji.cn/user/c.asp'
  # å‚æ•°
  data = {
      'u': 'action',
      'p': 'action123',
      'sn': '1',
      't': 'big',
  }
  # è¯·æ±‚å¤´
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36'
  }
  # è®¾ç½®session
  session = requests.session()
  # å½“é€šè¿‡sessionå»è®¿é—®çš„æ—¶å€™ é‚£ä¹ˆä¼šå°†å“åº”çš„æ‰€æœ‰çš„æ•°æ®ç»‘å®šå†sessionä¸Š
  session.get(url=url_get,params=data,headers=headers)
  # è¦è®¿é—®çš„ç½‘å€
  url = 'http://www.jokeji.cn/User/MemberCenter.asp'
  # å†æ¬¡é€šè¿‡sessionå»è®¿é—® é‚£ä¹ˆå°†ä¼šå°†ä¹‹å‰è¿”å›çš„ä¿¡æ¯æäº¤ç»™è¯¥è¯·æ±‚
  response = session.get(url=url,headers=headers)
  # requestsé»˜è®¤çš„ç¼–ç æ ¼å¼æ˜¯iso-8859-1
  response.encoding = 'gb2312'
  # è·å–ç½‘ç«™æºç 
  content = response.text
  # ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ä¸­
  with open('xh.html','w',encoding='gb2312')as fp:
      fp.write(content)
  ```

+ å…¨ä¹¦ç½‘

  ```python
  import requests
  # è¦è®¿é—®çš„ç½‘å€
  url = 'http://www.quanshuwang.com/login.php?do=submit'
  # å‚æ•°
  data = {
      'username': 'action',
      'password': 'action',
      'action': 'login',
  }
  # è¯·æ±‚å¤´
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36'
  }
  # è®¾ç½®session
  session = requests.session()
  # å½“é€šè¿‡sessionå»è®¿é—®çš„æ—¶å€™ é‚£ä¹ˆä¼šå°†å“åº”çš„æ‰€æœ‰çš„æ•°æ®ç»‘å®šå†sessionä¸Š
  session.post(url=url,data=data,headers=headers)
  # è¦è®¿é—®çš„ç½‘å€
  url_get = 'http://www.quanshuwang.com/modules/article/bookcase.php'
  # å†æ¬¡é€šè¿‡sessionå»è®¿é—® é‚£ä¹ˆå°†ä¼šå°†ä¹‹å‰è¿”å›çš„ä¿¡æ¯æäº¤ç»™è¯¥è¯·æ±‚
  response = session.get(url=url_get,headers=headers)
  # ç¼–ç 
  response.encoding = 'gbk'
  # è·å–ç½‘ç«™æºç  
  content = response.text
  # ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ä¸­
  with open('qs.html','w',encoding='gbk')as fp:
      fp.write(content)
  ```

+ å¤è¯—æ–‡ç½‘

  ```python
  import requests
  from bs4 import BeautifulSoup
  import urllib.request
  # è¦è®¿é—®çš„ç½‘å€
  url = 'https://so.gushiwen.org/user/login.aspx?from=http://so.gushiwen.org/user/collect.aspx'
  # è¯·æ±‚å¤´
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36'
  }
  # è®¾ç½®session
  session = requests.session()
  # è®¿é—®æµè§ˆå™¨
  response = requests.get(url=url,headers=headers)
  # ç™»é™†é¡µé¢çš„æºç 
  content = response.text
  # ç”¨bs4è§£æ
  soup = BeautifulSoup(content,'lxml')
  # è§£æè‡ªå·±æƒ³è¦çš„æ•°æ®
  viewstate = soup.select('#__VIEWSTATE')[0].attrs.get('value')
  viewstategenerator = soup.select('#__VIEWSTATEGENERATOR')[0].attrs.get('value')
  # è·å–img
  code = soup.select('#imgCode')[0].attrs.get('src')
  # æ‹¼æ¥
  code_url = 'https://so.gushiwen.org' + code
  # 
  response_code = session.get(url=code_url)
  content_code=response_code.content
  with open('code.jpg','wb')as fp:
      fp.write(content_code)
  
  codename = input('è¯·è¾“å…¥éªŒè¯ç ')
  # æäº¤è¡¨å•
  url_post = 'https://so.gushiwen.org/user/login.aspx?from=http%3a%2f%2fso.gushiwen.org%2fuser%2fcollect.aspx'
  data_post = {
      '__VIEWSTATE': viewstate,
      '__VIEWSTATEGENERATOR': viewstategenerator,
      'from': 'http://so.gushiwen.org/user/collect.aspx',
      'email': '595165358@qq.com',
      'pwd': 'action',
      'code': codename,
      'denglu': 'ç™»å½•',
  }
  response_post = session.post(url=url,headers=headers,data=data_post)
  content_post = response_post.text
  with open('gushiwen.html','w',encoding='utf-8')as fp:
      fp.write(content_post)
  ```


## 6. scrapy

### 1. scrapyåŸºæœ¬çŸ¥è¯†

```python
1.scrapyæ¦‚å¿µ
	Scrapyæ˜¯ä¸€ä¸ªä¸ºäº†çˆ¬å–ç½‘ç«™æ•°æ®ï¼Œæå–ç»“æ„æ€§æ•°æ®è€Œç¼–å†™çš„åº”ç”¨æ¡†æ¶ã€‚ å¯ä»¥åº”ç”¨åœ¨åŒ…æ‹¬æ•°æ®æŒ–æ˜ï¼Œä¿¡æ¯å¤„ç†æˆ–å­˜å‚¨å†å²æ•°æ®ç­‰ä¸€ç³»åˆ—çš„ç¨‹åºä¸­ã€‚
2.å®‰è£…scrapy
	1.pip install wheel
	2.pip install Twisted
	3.pip install lxml
	4.pip install pypiwin32
	5.pip install scrapy
3.scrapyé¡¹ç›®åˆ›å»º
	1.scrapy startproject  é¡¹ç›®åç§°
4.é¡¹ç›®ç»„æˆ
	spiders 
    	__init__.py
        è‡ªå®šä¹‰çš„çˆ¬è™«æ–‡ä»¶.py	===>ç”±æˆ‘ä»¬è‡ªå·±åˆ›å»ºï¼Œæ˜¯å®ç°çˆ¬è™«æ ¸å¿ƒåŠŸèƒ½çš„æ–‡ä»¶
    __init__.py                  
    items.py             ===>å®šä¹‰æ•°æ®ç»“æ„çš„åœ°æ–¹ï¼Œæ˜¯ä¸€ä¸ªç»§æ‰¿è‡ªscrapy.Itemçš„ç±»
    middlewares.py       ===>ä¸­é—´ä»¶   ä»£ç†
    pipelines.py		 ===>ç®¡é“æ–‡ä»¶ï¼Œé‡Œé¢åªæœ‰ä¸€ä¸ªç±»ï¼Œç”¨äºå¤„ç†ä¸‹è½½æ•°æ®çš„åç»­å¤„ç†
		é»˜è®¤æ˜¯300ä¼˜å…ˆçº§ï¼Œå€¼è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼ˆ1-1000ï¼‰
    settings.py			 ===>é…ç½®æ–‡ä»¶  æ¯”å¦‚ï¼šéµå®ˆrobotsåè®®ï¼ŒUser-Agentå®šä¹‰
5.åˆ›å»ºçˆ¬è™«æ–‡ä»¶
	1.è·³è½¬åˆ°spidersæ–‡ä»¶å¤¹  cd ç›®å½•åå­—/ç›®å½•åå­—/spiders
	2.scrapy genspider çˆ¬è™«åå­— ç½‘é¡µçš„åŸŸå
	3.çˆ¬è™«æ–‡ä»¶çš„ç»„æˆ
		1.name = 'bd'# çˆ¬è™«çš„æ–‡ä»¶çš„åå­—
		2.allowed_domains = ['http://www.baidu.com']# å…è®¸è®¿é—®çš„åŸŸå
		3.start_urls = ['http://www.baidu.com/']# èµ·å§‹çš„url
		4.def parse(self, response)
        	1.response.text è¯»å–ç½‘é¡µçš„
            2.response.body ä»¥äºŒè¿›åˆ¶å½¢å¼è¯»å–ç½‘é¡µ
            3.response.xpath() ç”¨xpathè§£æç½‘é¡µ
            	1.extract()æå–çš„æ˜¯selectorå¯¹è±¡çš„æ˜¯data
                2.extract_first()æå–çš„æ˜¯selectoråˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªæ•°æ®
6.è¿è¡Œçˆ¬è™«æ–‡ä»¶
	scrapy crawl çˆ¬è™«åç§° # æ³¨æ„ï¼šåº”åœ¨spidersæ–‡ä»¶å¤¹å†…æ‰§è¡Œ
7.æ‰©å±•ï¼šå¯¼å‡ºæ–‡ä»¶
	-o name.json
	-o name.xml
	-o name.csv
```

### 2. scrapyå·¥ä½œåŸç†

![](G:\å­¦ä¹ èµ„æ–™\ç¬”è®°\Typora\æ€»ç»“ç¬”è®°\img\scrapyåŸç†.png)

![](G:\å­¦ä¹ èµ„æ–™\ç¬”è®°\Typora\æ€»ç»“ç¬”è®°\img\scrapyåŸç†_è‹±æ–‡.png)

### 3. scrapy shell

```python
1.å®‰è£…ipython
	pip install ipython
2.åº”ç”¨ï¼š1.scrapy shell www.baidu.com
       2.scrapy shell http://www.baidu.com
       3.scrapy shell "http://www.baidu.com"
       4.scrapy shell "www.baidu.com"
3.è¯­æ³•
	1.responseå¯¹è±¡
		1.response.body	# ä»¥äºŒè¿›åˆ¶å½¢å¼è¯»å–ç½‘é¡µ
        2.response.text	# è¯»å–ç½‘é¡µ
        3.response.url		# è¯·æ±‚è·¯ç”±
        4.response.status	# çŠ¶æ€ç 
	2.responseçš„è§£æ
    	1.response.xpath() ä½¿ç”¨xpathè·¯å¾„æŸ¥è¯¢ç‰¹å®šå…ƒç´ ï¼Œè¿”å›ä¸€ä¸ªselectoråˆ—è¡¨å¯¹è±¡
        2.response.css()ä½¿ç”¨css_selectoræŸ¥è¯¢å…ƒç´ ï¼Œè¿”å›ä¸€ä¸ªselectoråˆ—è¡¨å¯¹è±¡
        	è·å–å†…å®¹ ï¼šresponse.css('#su::text').extract_first()
            è·å–å±æ€§ ï¼šresponse.css('#su::attr(â€œvalueâ€)').extract_first()
	3.selectorå¯¹è±¡ï¼ˆé€šè¿‡xpathæ–¹æ³•è°ƒç”¨è¿”å›çš„æ˜¯seletoråˆ—è¡¨ï¼‰
    	1.extract()æå–çš„æ˜¯selectorå¯¹è±¡çš„æ˜¯data
        2.extract_first()æå–çš„æ˜¯selectoråˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªæ•°æ®
    4.itemå¯¹è±¡
    	1.dict(itemobj)å¯ä»¥ä½¿ç”¨dictæ–¹æ³•ç›´æ¥å°†itemå¯¹è±¡è½¬æ¢æˆå­—å…¸å¯¹è±¡
        2.Item(dicobj) å¯ä»¥ä½¿ç”¨å­—å…¸å¯¹è±¡åˆ›å»ºä¸€ä¸ªItemå¯¹è±¡
```

## 7. yield

### 1. yieldåŸºç¡€ä¸ä¾‹å­

```python
1.å¸¦æœ‰yieldçš„å‡½æ•°ä¸å†æ˜¯ä¸€ä¸ªæ™®é€šå‡½æ•°ï¼Œè€Œæ˜¯ä¸€ä¸ªç”Ÿæˆå™¨generatorï¼Œå¯ç”¨äºè¿­ä»£
2.yield æ˜¯ä¸€ä¸ªç±»ä¼¼returnçš„å…³é”®å­—ï¼Œè¿­ä»£ä¸€æ¬¡é‡åˆ°yieldæ—¶å°±è¿”å›yieldåé¢(å³è¾¹)çš„å€¼ã€‚é‡ç‚¹æ˜¯ï¼šä¸‹ä¸€æ¬¡è¿­ä»£æ—¶ï¼Œä»ä¸Šä¸€æ¬¡è¿­ä»£é‡åˆ°çš„yieldåé¢çš„ä»£ç (ä¸‹ä¸€è¡Œ)å¼€å§‹æ‰§è¡Œ
3.ç®€è¦ç†è§£ï¼šyieldå°±æ˜¯returnè¿”å›ä¸€ä¸ªå€¼ï¼Œå¹¶ä¸”è®°ä½è¿™ä¸ªè¿”å›çš„ä½ç½®ï¼Œä¸‹æ¬¡è¿­ä»£å°±ä»è¿™ä¸ªä½ç½®å(ä¸‹ä¸€è¡Œ)å¼€å§‹
```

```python
name_list = [x for x in range(10)]
def createGenorator():
	items = []
	for i in name_list:
		print('ç¬¬{}æ¬¡è°ƒç”¨'.format(i))
		items.append(i)
	return items
def testFunc1():
	generator = createGenorator2()
	for a in generator:
		print('ä½¿ç”¨ç¬¬{}æ¬¡'.format(a))
def createGenorator2():
	for i in name_list:
		print('ç¬¬{}æ¬¡è°ƒç”¨'.format(i))
		yield i
print(testFunc1())
```

### 2. scrapyçˆ¬å–å›¾ç‰‡â€”â€”ç«™é•¿ç´ æ(å¤šç®¡é“å¤šé¡µä¸‹è½½)

```python
##################è‡ªå®šä¹‰çš„çˆ¬è™«æ–‡ä»¶.py################
# -*- coding: utf-8 -*-
import scrapy
from ..items import ZhanzhangItem

class ZzSpider(scrapy.Spider):
    name = 'zz'	# çˆ¬è™«æ–‡ä»¶åå­—
    allowed_domains = ['sc.chinaz.com']		# çˆ¬å–ç½‘é¡µçš„æ ¹è·¯ç”±
    start_urls = ['http://sc.chinaz.com/tupian/shanshuifengjing.html']# å¼€å§‹çˆ¬å–çš„ç½‘é¡µ
    page = 1 
    base_url = 'http://sc.chinaz.com/tupian/shanshuifengjing_'
    def parse(self, response):
        img_list = response.xpath('//div[@id="container"]//a/img')
        for img in img_list:
            src = img.xpath('./@src2').extract_first()
            alt = img.xpath('./@alt').extract_first()
            zz = ZhanzhangItem(src=src,alt=alt)
            yield zz
		# æŠŠå‰5é¡µçš„url æ‰¾åˆ°
        if self.page <= 5:
            self.page = self.page + 1
            url = self.base_url + str(self.page) + '.html'
            print('=========================')
            print(url)
            # callbackçš„å€¼ ä¸åŠ ï¼ˆï¼‰
            yield scrapy.Request(url=url,callback=self.parse)
####################items.py########################
# -*- coding: utf-8 -*-
import scrapy
class ZhanzhangItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    # å®šä¹‰æ•°æ®ç»“æ„
    src = scrapy.Field()
    alt = scrapy.Field()
###################pipelines.py##################
# -*- coding: utf-8 -*-
import urllib.request
# å†™å…¥zz.jsonä¸­
class ZhanzhangPipeline(object):
	# çˆ¬è™«å¼€å§‹æ‰§è¡Œçš„
    def open_spider(self,spider):
        self.fp = open('zz.json','w',encoding='utf-8')
    # çˆ¬è™«æ‰§è¡Œæ—¶æ‰§è¡Œçš„
    def process_item(self, item, spider):
        self.fp.write(str(item))
        return item
	# çˆ¬è™«ç»“æŸæ‰§è¡Œçš„
    def close_spider(self,spdier):
        self.fp.close()
# ä¸‹è½½å›¾ç‰‡
class ZhanzhangDownLoadPipeline(object):
    def process_item(self,item,spider):
        src = item['src']
        alt = item['alt']
        filename = './mw/'+alt+'.jpg'
        urllib.request.urlretrieve(url=src,filename=filename)
        return item
######################settings.py####################
ITEM_PIPELINES = {
   'zhanzhang.pipelines.ZhanzhangPipeline': 300,
   'zhanzhang.pipelines.ZhanzhangDownLoadPipeline':250,
}
```

### 3. scrapyçˆ¬å–ä¸€çº§ç•Œé¢çš„æ ‡é¢˜å’ŒäºŒçº§ç•Œé¢çš„å›¾ç‰‡â€”â€”ç”µå½±å¤©å ‚

```python
##############è‡ªå®šä¹‰çš„çˆ¬è™«æ–‡ä»¶.py#################
# -*- coding: utf-8 -*-
import scrapy
from ..items import MovieItem

class MvSpider(scrapy.Spider):
    name = 'mv'
    allowed_domains = ['www.ygdy8.net']
    start_urls = ['https://www.ygdy8.net/html/gndy/dyzz/index.html']

    def parse(self, response):
        a_list = response.xpath('//div[@class="co_content8"]//b/a')
        for a in a_list:
            name = a.xpath('./text()').extract_first()
            href = a.xpath('./@href').extract_first()
            url = 'https://www.ygdy8.net' + href
            yield scrapy.Request(url=url,callback=self.parse_second,meta={'name':name})

    def parse_second(self,response):
        name = response.meta['name']
        src = response.xpath('//div[@id="Zoom"]//img[1]/@src').extract_first()
        movie = MovieItem(name=name,src=src)
        yield movie
###################items.py###########################
# -*- coding: utf-8 -*-
import scrapy

class MovieItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    # å®šä¹‰æ•°æ®ç»“æ„
    name = scrapy.Field()
    src = scrapy.Field()
###################pipelines.py#########################
class MoviePipeline(object):
    def open_spider(self,spider):
        self.fp = open('movie.json','w',encoding='utf-8')
    def process_item(self, item, spider):
        self.fp.write(str(item))
        return item
    def close_spider(self,spider):
        self.fp.close()
```

### 4. MySQL

```
1.ä¸‹è½½
	https://dev.mysql.com/downloads/windows/installer/5.7.html
2.å®‰è£…
	https://jingyan.baidu.com/album/d7130635f1c77d13fdf475df.html
3.pymysqlçš„ä½¿ç”¨
	1.pip install pymysql
	2.conn = pymysql.Connect(host,port,user,password,db,charset)
	3.conn.cursor()
	4.cursor.execute(sql)
	5.conn.commit()
    6.cursor.close()
    7.conn.close()
```

## 8. CrawlSpiderâ€”è·Ÿè¿›æå–é“¾æ¥

### 1. CrawlSpideråŸºç¡€çŸ¥è¯†

```
1.ç»§æ‰¿è‡ªscrapy.Spider
2.ç‹¬é—¨ç§˜ç¬ˆ
	CrawlSpiderå¯ä»¥å®šä¹‰è§„åˆ™ï¼Œå†è§£æhtmlå†…å®¹çš„æ—¶å€™ï¼Œå¯ä»¥æ ¹æ®é“¾æ¥è§„åˆ™æå–å‡ºæŒ‡å®šçš„é“¾æ¥ï¼Œç„¶åå†å‘è¿™äº›é“¾æ¥å‘é€è¯·æ±‚ã€‚æ‰€ä»¥ï¼Œå¦‚æœæœ‰éœ€è¦è·Ÿè¿›é“¾æ¥çš„éœ€æ±‚ï¼Œæ„æ€å°±æ˜¯çˆ¬å–äº†ç½‘é¡µä¹‹åï¼Œéœ€è¦æå–é“¾æ¥å†æ¬¡çˆ¬å–ï¼Œä½¿ç”¨CrawlSpideræ˜¯éå¸¸åˆé€‚çš„
3.æå–é“¾æ¥
	é“¾æ¥æå–å™¨ï¼Œåœ¨è¿™é‡Œå°±å¯ä»¥å†™è§„åˆ™æå–æŒ‡å®šé“¾æ¥
	scrapy.linkextractors.LinkExtractor(
	 	allow = (),           # æ­£åˆ™è¡¨è¾¾å¼  æå–ç¬¦åˆæ­£åˆ™çš„é“¾æ¥
	 	deny = (),            # (ä¸ç”¨)æ­£åˆ™è¡¨è¾¾å¼  ä¸æå–ç¬¦åˆæ­£åˆ™çš„é“¾æ¥
	 	allow_domains = (),   # ï¼ˆä¸ç”¨ï¼‰å…è®¸çš„åŸŸå
		deny_domains = (),    # ï¼ˆä¸ç”¨ï¼‰ä¸å…è®¸çš„åŸŸå
	 	restrict_xpaths = (), # xpathï¼Œæå–ç¬¦åˆxpathè§„åˆ™çš„é“¾æ¥
	 	restrict_css = ()     # æå–ç¬¦åˆé€‰æ‹©å™¨è§„åˆ™çš„é“¾æ¥)
4.æ¨¡æ‹Ÿä½¿ç”¨
		æ­£åˆ™ç”¨æ³•ï¼šlinks1 = LinkExtractor(allow=r'list_23_\d+\.html')
		xpathç”¨æ³•ï¼šlinks2 = LinkExtractor(restrict_xpaths=r'//div[@class="x"]')
		cssç”¨æ³•ï¼šlinks3 = LinkExtractor(restrict_css='.x')
5.æå–è¿æ¥
		link.extract_links(response)
################éœ€æ±‚ï¼šè¯»ä¹¦ç½‘æ•°æ®å…¥åº“###############
6.åˆ›å»ºé¡¹ç›®ï¼šscrapy startproject dushuproject
7.è·³è½¬åˆ°spidersè·¯å¾„  cd\dushuproject\dushuproject\spiders
8.åˆ›å»ºçˆ¬è™«ç±»ï¼šscrapy genspider -t crawl read www.dushu.com
9.items
10.spiders
11.settings
12.pipelines
		æ•°æ®ä¿å­˜åˆ°æœ¬åœ°
		æ•°æ®ä¿å­˜åˆ°mysqlæ•°æ®åº“
```

### 2. CrawlSpiderè·Ÿè¿›æå–é“¾æ¥â€”â€”è¯»ä¹¦ç½‘

```python
################è‡ªå®šä¹‰çš„çˆ¬è™«æ–‡ä»¶.py######################
# -*- coding: utf-8 -*-
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from ..items import ReadItem

class ReadbookSpider(CrawlSpider):
    name = 'readbook'
    allowed_domains = ['www.dushu.com']
    start_urls = ['https://www.dushu.com/book/1107_1.html']

    # ä¼šæå–å‡ºå½“å‰é¡µé¢æ‰€æœ‰ç¬¦åˆè§„åˆ™è¿æ¥
    rules = (
        Rule(LinkExtractor(allow=r'/book/1107_\d+.html'),
                           callback='parse_item',
                           follow=True),
    )

    def parse_item(self, response):
        img_list = response.xpath('//div[@class="bookslist"]//img')
        for img in img_list:
            src = img.xpath('./@data-original').extract_first()
            name = img.xpath('./@alt').extract_first()
            book = ReadItem(src=src,name=name)
            yield book
###############items.py###########################
# -*- coding: utf-8 -*-
import pymysql
from scrapy.utils.project import get_project_settings
import scrapy
class ReadItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    src = scrapy.Field()
    name = scrapy.Field()
##################pipelines.py#########################
# -*- coding: utf-8 -*-
class ReadPipeline(object):
    def open_spider(self,spider):
        self.fp = open('book.json','w',encoding='utf-8')
    def process_item(self, item, spider):
        self.fp.write(str(item))
      	return item
    def close_spider(self,spider):
        self.fp.close()

class ReadMysqlPipeline(object):
    def open_spider(self,spider):
    	# å»ºç«‹æ•°æ®åº“çš„è¿æ¥
        self.conn = self.getconn()
        self.cursor = self.conn.cursor()
    def getconn(self):
        settings = get_project_settings()
        conn = pymysql.Connect(host=settings['DB_HOST'],
                        user=settings['DB_USER'],
                        password=settings['DB_PASSWORD'],
                        database=settings['DB_DATABASE'],
                        # ç«¯å£å·æ˜¯æ•´æ•°
                        port=settings['DB_PORT'],
                        # charset=utf8 æ²¡æœ‰-
                        charset=settings['DB_CHARSET'])
        return conn
    def process_item(self,item,spider):
        sql = 'insert into book1905 values ("{}","{}")'.format(item['src'],item['name'])
        self.cursor.execute(sql)
        self.conn.commit()
        return item
    def close_spider(self,spider):
        # å…³é—­æ•°æ®åº“çš„è¿æ¥
        self.cursor.close()
        self.conn.close()
####################settings.py######################
DB_HOST='192.168.231.140'	# é“¾æ¥çš„ä¸»æœº
DB_USER='root'				# ç”¨æˆ·
DB_PASSWORD='1234'			# å¯†ç 
DB_DATABASE='spider1905'	# æ•°æ®åº“å
DB_PORT=3306				# ç«¯å£å·
DB_CHARSET='utf8'			# ç¼–ç 
```

## 9. æ—¥å¿—ä¿¡æ¯å’Œæ—¥å¿—ç­‰çº§

```
1.æ—¥å¿—çº§åˆ«
	1.CRITICALï¼šä¸¥é‡é”™è¯¯
    2.ERRORï¼šä¸€èˆ¬é”™è¯¯
    3.WARNINGï¼šè­¦å‘Š
    4.INFO: ä¸€èˆ¬ä¿¡æ¯
    5.DEBUGï¼šè°ƒè¯•ä¿¡æ¯
    æç¤ºï¼šé»˜è®¤çš„æ—¥å¿—ç­‰çº§æ˜¯DEBUG,åªè¦å‡ºç°äº†DEBUGæˆ–è€…DEBUGä»¥ä¸Šç­‰çº§çš„æ—¥å¿—,é‚£ä¹ˆè¿™äº›æ—¥å¿—å°†ä¼šæ‰“å°
2.settings.pyæ–‡ä»¶è®¾ç½®
	LOG_FILE:å°†å±å¹•æ˜¾ç¤ºçš„ä¿¡æ¯å…¨éƒ¨è®°å½•åˆ°æ–‡ä»¶ä¸­ï¼Œå±å¹•ä¸å†æ˜¾ç¤ºï¼Œæ³¨æ„æ–‡ä»¶åç¼€ä¸€å®šæ˜¯.log
	LOG_LEVEL:è®¾ç½®æ—¥å¿—æ˜¾ç¤ºçš„ç­‰çº§ï¼Œå°±æ˜¯æ˜¾ç¤ºå“ªäº›ï¼Œä¸æ˜¾ç¤ºå“ªäº›
```

## 10. scrapyçš„getå’Œpost

### 1. scrapyçš„get

```python
1.scrapy.Request(url=url, callback=self.parse_item, meta={'item': item}, headers=headers)
	url: è¦è¯·æ±‚çš„åœ°å€
    callbackï¼šå“åº”æˆåŠŸä¹‹åçš„å›è°ƒå‡½æ•°
    meta: å‚æ•°ä¼ é€’  æ¥æ”¶çš„è¯­æ³•ï¼šitem = response.meta['item']
    headers: å®šåˆ¶å¤´ä¿¡æ¯ï¼Œä¸€èˆ¬ä¸ç”¨      
    parse_itemæ–¹æ³•ä¸­çš„responseå‚æ•°å°±æ˜¯urlæ‰§è¡Œä¹‹åçš„è¯·æ±‚ç»“æœ
2.response æ˜¯ä¸€ä¸ªå¯¹è±¡ å‡½æ•°çš„ç¬¬äºŒä¸ªå‚æ•°
    response.text: å­—ç¬¦ä¸²æ ¼å¼çš„æ–‡æœ¬
    response.body: äºŒè¿›åˆ¶æ ¼å¼çš„æ–‡æœ¬
    response.url: å½“å‰å“åº”çš„urlåœ°å€
    response.status: çŠ¶æ€ç 
    response.xpath(): ç­›é€‰ä½ æƒ³è¦çš„å†…å®¹
    response.css(): ç­›é€‰ä½ æƒ³è¦çš„å†…å®¹
```

### 2. scrapyçš„postè¯·æ±‚

```python
1.é‡å†™start_requestsæ–¹æ³•ï¼š
		def start_requests(self)
2.start_requestsçš„è¿”å›å€¼ï¼š
 	 scrapy.FormRequest(url=url, headers=headers, callback=self.parse_item,formdata=data)
         url: è¦å‘é€çš„poståœ°å€
         headersï¼šå¯ä»¥å®šåˆ¶å¤´ä¿¡æ¯
         callback: å›è°ƒå‡½æ•°   
         formdata: postæ‰€æºå¸¦çš„æ•°æ®ï¼Œè¿™æ˜¯ä¸€ä¸ªå­—å…¸
```

### 3. å…ˆpostè¯·æ±‚å†è·³è½¬åˆ°getè¯·æ±‚

```python
#######################è‡ªå®šä¹‰çš„çˆ¬è™«æ–‡ä»¶.py######################
# -*- coding: utf-8 -*-
import scrapy

class WbSpider(scrapy.Spider):
    name = 'wb'
    allowed_domains = ['weibo.cn']
	# ç™»å½•é¡µé¢
    def start_requests(self):
        # è·¯ç”±
        url = 'https://passport.weibo.cn/sso/login'
        # è¯·æ±‚å¤´        
        headers={
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.9',
            'Connection': 'keep-alive',
            'Content-Type': 'application/x-www-form-urlencoded',
            'Cookie': 'SCF=Ahi2Sm3XHpcYIJvIsbJd8AnqkyO8t5RFmHXn8yHeTOMYgumvEqFGsgNbZbD6BmzlV7GA-B8sNWcbTcHeVmF3eNc.; _T_WM=1f1272ae0786a2a2ef34c961901b618b; SUHB=0rrLDdNlZe5bVf; login=d434df472bb5ab59af1d4577b2b5d916',
            'Host': 'passport.weibo.cn',
            'Origin': 'https://passport.weibo.cn',
            'Referer': 'https://passport.weibo.cn/signin/login?entry=mweibo&r=https%3A%2F%2Fweibo.cn%2F&backTitle=%CE%A2%B2%A9&vt=',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36',
        }
        # å‚æ•°        
        data={
            'username': '18642820892',
            'password': 'lijing1150501',
            'savestate': '1',
            'r': 'https://weibo.cn/',
            'ec': '0',
            'pagerefer': 'https://weibo.cn/pub/?vt=',
            'entry': 'mweibo',
            'wentry': '',
            'loginfrom': '',
            'client_id': '',
            'code': '',
            'qq': '',
            'mainpageflag': '1',
            'hff': '',
            'hfp': '',
        }
        # å‘å‡ºç™»å½•postè¯·æ±‚        
        yield scrapy.FormRequest(url=url,formdata=data,headers=headers,callback=self.parse_second)
	# ç™»å½•æˆåŠŸä¹‹å
    def parse_second(self,response):
        # url        
        url = 'https://weibo.cn/6451491586/info'
		# å‘å‡ºä¿®æ”¹getè¯·æ±‚
        yield scrapy.Request(url=url,callback=self.parse_third)
	# æŠŠè·å–çš„æ•°æ®ä¿å­˜èµ·æ¥
    def parse_third(self,response):
        content = response.text
        with open('wb.html','w',encoding='utf-8')as fp:
            fp.write(content)
##########################settings.py############################
# -*- coding: utf-8 -*-
BOT_NAME = 'weibo'

SPIDER_MODULES = ['weibo.spiders']
NEWSPIDER_MODULE = 'weibo.spiders'

# Crawl responsibly by identifying yourself (and your website) on the user-agent
USER_AGENT = 'User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36'

# Obey robots.txt rules
ROBOTSTXT_OBEY = False
```

## 11. ä»£ç†ï¼ˆé€šè¿‡ä¸‹è½½ä¸­é—´ä»¶æ¥è¿›è¡Œæ·»åŠ ï¼‰

```python
1.åˆ°settings.pyä¸­ï¼Œæ‰“å¼€ä¸€ä¸ªé€‰é¡¹
	DOWNLOADER_MIDDLEWARES = {'postproject.middlewares.Proxy': 543,}
2.åˆ°middlewares.pyä¸­å†™ä»£ç 
	def process_request(self, request, spider):
		request.meta['proxy'] = 'https://113.68.202.10:9999'
		return None
```

## 12. scrapy-redis

### 1. rediså®‰è£…â€”â€” Redis-x64-3.2.100 

```python
1.æŠŠå‹ç¼©åŒ…è§£å‹åˆ°Eç›˜
2.å¯åŠ¨
	1.E:
	2.cd Redis\Redis-x64-3.2.100
	3.redis-server redis.windows.conf
	4.è®¾ç½®æœåŠ¡å‘½ä»¤redis-server --service-install redis.windows-service.conf --loglevel verbose
3.è¿æ¥
	1.æœ¬åœ°è¿æ¥ redis-cli
	2.è¿œç¨‹è¿æ¥ ç¬¬56è¡Œç›´æ¥æ³¨é‡Šæ‰ ç¬¬75è¡Œæ”¹ä¸ºprotected-mode no
	3. redis-cli -h host -p port      
	   redis-cli -h 10.11.63.79
4.å®¢æˆ·ç«¯å›¾å½¢åŒ–å®‰è£…
	å›¾å½¢åŒ–ç•Œé¢æ“ä½œredisæ•°æ®åº“
	select 0-15 ç”¨æ¥åˆ‡æ¢æ•°æ®åº“	
æ‰©å±•ï¼š
	å¸è½½æœåŠ¡ï¼šredis-server --service-uninstall
	å¼€å¯æœåŠ¡ï¼šredis-server --service-start
	åœæ­¢æœåŠ¡ï¼šredis-server --service-stop
	å®˜ç½‘ä¸‹è½½åœ°å€ï¼šhttp://redis.io/download
	githubä¸‹è½½åœ°å€ï¼šhttps://github.com/MSOpenTech/redis/tags
```

### 2. scrapyå’Œscrapy_redisåŒºåˆ«ï¼Ÿ

```
1.scrapyæ˜¯ä¸€ä¸ªé€šç”¨çš„çˆ¬è™«æ¡†æ¶ï¼Œä½†æ˜¯è¿™ä¸ªæ¡†æ¶ä¸æ”¯æŒåˆ†å¸ƒå¼
2.scrapy_rediså°±æ˜¯ä¸ºäº†å®ç°scrapyçš„åˆ†å¸ƒå¼è€Œè¯ç”Ÿçš„ï¼Œå®ƒæä¾›äº†ä¸€äº›åŸºäºredisçš„ç»„ä»¶
	ç½‘å€ï¼šhttps://www.cnblogs.com/nick477931661/p/9135497.html
```

### 3. scrapy_rediså®˜æ–¹

ï¼ˆhttps://github.com/rmax/scrapy-redisï¼‰

```python
1.ç»§æ‰¿äº†RedisSpider
2.from scrapy_redis.spiders import RedisSpider
3.æ·»åŠ ä¸€ä¸ªredis_key = 'myspider:start_urls'
4.æ³¨æ„å®˜ç½‘æä¾›çš„initæ–¹æ³•ä¸å¥½ç”¨ï¼Œæ‰€ä»¥è¿˜éœ€è¦æˆ‘ä»¬è‡ªå·±æ‰‹åŠ¨æ·»åŠ allowed_domains
5.åœ¨settingsä¸­æ·»åŠ 
	# æŒ‡çº¹å»é‡
	DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
	# è°ƒåº¦å™¨ç»„ä»¶
	SCHEDULER = "scrapy_redis.scheduler.Scheduler"
	# åœ¨çˆ¬å–çš„è¿‡ç¨‹ä¸­å…è®¸æš‚åœ
	SCHEDULER_PERSIST = True
	# æŒ‡å®šè¿æ¥çš„æ•°æ®åº“ å¦‚æœæœ¬åœ°è¿æ¥å°±ä¸éœ€è¦å†™äº†
	REDIS_HOST = '10.11.52.62' masterç«¯çš„ä¸»æœºåœ°å€
	REDIS_PORT = 6379
6.åœ¨settingsçš„ç®¡é“ä¸­æ·»åŠ 
	'scrapy_redis.pipelines.RedisPipeline': 400,ä¼šå°†æ•°æ®æ·»åŠ åˆ°redisæ•°æ®åº“ä¸­
7.DOWNLOAD_DELAY = 1 # åœ¨çˆ¬å–ç½‘ç«™çš„æ—¶å€™ï¼Œå°†è¿™ä¸ªé€‰é¡¹æ‰“å¼€ï¼Œç»™å¯¹æ–¹ä¸€æ¡æ´»è·¯
8.slaveç«¯æ‰§è¡Œ scrapy runspider mycrawler_redis.py
9.masterç«¯å‘é˜Ÿåˆ—ä¸­æ·»åŠ èµ·å§‹url
	è¿™ä¸ªkeyå°±æ˜¯ä½ ä»£ç ä¸­å†™çš„  redis_key
	lpush fen:start_urls 'http://www.dytt8.net/html/gndy/dyzz/index.html'
```

